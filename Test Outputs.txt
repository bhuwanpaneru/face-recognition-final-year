Dataset : LFW (First 1000 persons - Non-uniform samples for each person : JPG format) 
-------------------------------------------------------------------------
Images length : 1936
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_13 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_13 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_14 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 1000)              513000    
_________________________________________________________________
activation_15 (Activation)   (None, 1000)              0         
=================================================================
Total params: 12,296,168
Trainable params: 12,296,168
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 1742 samples, validate on 194 samples
Epoch 1/50
1742/1742 [==============================] - 30s - loss: 8.8760 - acc: 0.0195 - val_loss: 6.7985 - val_acc: 0.0412
Epoch 2/50
1742/1742 [==============================] - 18s - loss: 6.5489 - acc: 0.0344 - val_loss: 6.8541 - val_acc: 0.0361
Epoch 3/50
1742/1742 [==============================] - 18s - loss: 6.4099 - acc: 0.0373 - val_loss: 7.0969 - val_acc: 0.0361
Epoch 4/50
1742/1742 [==============================] - 21s - loss: 6.4117 - acc: 0.0356 - val_loss: 7.1760 - val_acc: 0.0361
Epoch 5/50
1742/1742 [==============================] - 18s - loss: 6.3337 - acc: 0.0356 - val_loss: 7.1373 - val_acc: 0.0309
Epoch 6/50
1742/1742 [==============================] - 18s - loss: 6.2617 - acc: 0.0373 - val_loss: 7.3323 - val_acc: 0.0309
Epoch 7/50
1742/1742 [==============================] - 23s - loss: 6.2717 - acc: 0.0419 - val_loss: 7.5072 - val_acc: 0.0361
Epoch 8/50
1742/1742 [==============================] - 18s - loss: 6.1885 - acc: 0.0390 - val_loss: 7.3667 - val_acc: 0.0515
Epoch 9/50
1742/1742 [==============================] - 18s - loss: 6.1504 - acc: 0.0425 - val_loss: 7.5885 - val_acc: 0.0464
Epoch 10/50
1742/1742 [==============================] - 22s - loss: 6.0836 - acc: 0.0465 - val_loss: 8.1764 - val_acc: 0.0361
Epoch 11/50
1742/1742 [==============================] - 19s - loss: 6.0867 - acc: 0.0465 - val_loss: 8.1090 - val_acc: 0.0567
Epoch 12/50
1742/1742 [==============================] - 17s - loss: 6.0764 - acc: 0.0442 - val_loss: 8.0580 - val_acc: 0.0567
Epoch 13/50
1742/1742 [==============================] - 18s - loss: 6.0181 - acc: 0.0459 - val_loss: 7.9692 - val_acc: 0.0412
Epoch 14/50
1742/1742 [==============================] - 21s - loss: 5.9459 - acc: 0.0540 - val_loss: 7.9140 - val_acc: 0.0515
Epoch 15/50
1742/1742 [==============================] - 17s - loss: 5.9615 - acc: 0.0505 - val_loss: 8.8209 - val_acc: 0.0619
Epoch 16/50
1742/1742 [==============================] - 17s - loss: 5.9507 - acc: 0.0505 - val_loss: 8.2495 - val_acc: 0.0567
Epoch 17/50
1742/1742 [==============================] - 17s - loss: 5.9048 - acc: 0.0528 - val_loss: 8.1962 - val_acc: 0.0567
Epoch 18/50
1742/1742 [==============================] - 22s - loss: 5.8648 - acc: 0.0534 - val_loss: 8.4048 - val_acc: 0.0619
Epoch 19/50
1742/1742 [==============================] - 18s - loss: 5.8384 - acc: 0.0505 - val_loss: 8.4065 - val_acc: 0.0567
Epoch 20/50
1742/1742 [==============================] - 17s - loss: 5.8229 - acc: 0.0494 - val_loss: 8.6545 - val_acc: 0.0619
Epoch 21/50
1742/1742 [==============================] - 18s - loss: 5.7915 - acc: 0.0459 - val_loss: 8.4175 - val_acc: 0.0515
Epoch 22/50
1742/1742 [==============================] - 21s - loss: 5.7679 - acc: 0.0522 - val_loss: 8.5579 - val_acc: 0.0567
Epoch 23/50
1742/1742 [==============================] - 17s - loss: 5.7389 - acc: 0.0499 - val_loss: 8.7138 - val_acc: 0.0670
Epoch 24/50
1742/1742 [==============================] - 18s - loss: 5.7338 - acc: 0.0517 - val_loss: 8.9480 - val_acc: 0.0619
Epoch 25/50
1742/1742 [==============================] - 18s - loss: 5.7437 - acc: 0.0511 - val_loss: 8.8294 - val_acc: 0.0619
Epoch 26/50
1742/1742 [==============================] - 22s - loss: 5.7213 - acc: 0.0528 - val_loss: 8.6385 - val_acc: 0.0515
Epoch 27/50
1742/1742 [==============================] - 17s - loss: 5.6938 - acc: 0.0517 - val_loss: 9.0220 - val_acc: 0.0670
Epoch 28/50
1742/1742 [==============================] - 17s - loss: 5.7017 - acc: 0.0551 - val_loss: 8.9111 - val_acc: 0.0619
Epoch 29/50
1742/1742 [==============================] - 17s - loss: 5.7144 - acc: 0.0471 - val_loss: 9.3000 - val_acc: 0.0619
Epoch 30/50
1742/1742 [==============================] - 22s - loss: 5.7294 - acc: 0.0454 - val_loss: 8.7910 - val_acc: 0.0567
Epoch 31/50
1742/1742 [==============================] - 18s - loss: 5.6826 - acc: 0.0545 - val_loss: 8.9615 - val_acc: 0.0619
Epoch 32/50
1742/1742 [==============================] - 17s - loss: 5.6488 - acc: 0.0511 - val_loss: 8.9564 - val_acc: 0.0670
Epoch 33/50
1742/1742 [==============================] - 18s - loss: 5.6447 - acc: 0.0528 - val_loss: 9.0463 - val_acc: 0.0619
Epoch 34/50
1742/1742 [==============================] - 21s - loss: 5.6366 - acc: 0.0471 - val_loss: 9.0523 - val_acc: 0.0670
Epoch 35/50
1742/1742 [==============================] - 18s - loss: 5.6189 - acc: 0.0522 - val_loss: 8.9776 - val_acc: 0.0567
Epoch 36/50
1742/1742 [==============================] - 18s - loss: 5.6385 - acc: 0.0476 - val_loss: 9.2819 - val_acc: 0.0722
Epoch 37/50
1742/1742 [==============================] - 23s - loss: 5.6179 - acc: 0.0551 - val_loss: 9.2159 - val_acc: 0.0619
Epoch 38/50
1742/1742 [==============================] - 18s - loss: 5.5925 - acc: 0.0517 - val_loss: 9.2141 - val_acc: 0.0515
Epoch 39/50
1742/1742 [==============================] - 17s - loss: 5.6415 - acc: 0.0545 - val_loss: 9.1003 - val_acc: 0.0619
Epoch 40/50
1742/1742 [==============================] - 20s - loss: 5.5928 - acc: 0.0563 - val_loss: 9.3673 - val_acc: 0.0619
Epoch 41/50
1742/1742 [==============================] - 20s - loss: 5.5668 - acc: 0.0580 - val_loss: 9.0836 - val_acc: 0.0619
Epoch 42/50
1742/1742 [==============================] - 18s - loss: 5.5631 - acc: 0.0574 - val_loss: 9.2331 - val_acc: 0.0567
Epoch 43/50
1742/1742 [==============================] - 19s - loss: 5.6414 - acc: 0.0476 - val_loss: 9.1015 - val_acc: 0.0515
Epoch 44/50
1742/1742 [==============================] - 21s - loss: 5.5560 - acc: 0.0586 - val_loss: 9.3426 - val_acc: 0.0567
Epoch 45/50
1742/1742 [==============================] - 17s - loss: 5.5757 - acc: 0.0545 - val_loss: 8.9683 - val_acc: 0.0515
Epoch 46/50
1742/1742 [==============================] - 19s - loss: 5.6662 - acc: 0.0505 - val_loss: 9.2915 - val_acc: 0.0619
Epoch 47/50
1742/1742 [==============================] - 21s - loss: 5.5535 - acc: 0.0528 - val_loss: 9.2390 - val_acc: 0.0670
Epoch 48/50
1742/1742 [==============================] - 17s - loss: 5.5477 - acc: 0.0551 - val_loss: 9.4804 - val_acc: 0.0619
Epoch 49/50
1742/1742 [==============================] - 19s - loss: 5.5897 - acc: 0.0505 - val_loss: 9.2785 - val_acc: 0.0567
Epoch 50/50
1742/1742 [==============================] - 22s - loss: 5.5015 - acc: 0.0586 - val_loss: 9.3961 - val_acc: 0.0619
Loss : 9.39606029471
Accuracy :0.0618556701031


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Dataset : LFW (First 40 persons - Non-uniform samples for each person : JPG format) 
-------------------------------------------------------------------------
Images length : 76
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_16 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_17 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_18 (Dense)             (None, 40)                20520     
_________________________________________________________________
activation_18 (Activation)   (None, 40)                0         
=================================================================
Total params: 11,803,688
Trainable params: 11,803,688
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 68 samples, validate on 8 samples
Epoch 1/50
68/68 [==============================] - 6s - loss: 3.8813 - acc: 0.1471 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 2/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 3/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 4/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 5/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 6/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 7/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 8/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 9/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 10/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 11/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 12/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 13/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 14/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 15/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 16/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 17/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 18/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 19/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 20/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 21/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 22/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 23/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 24/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 25/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 26/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 27/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 28/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 29/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 30/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 31/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 32/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 33/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 34/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 35/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 36/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 37/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 38/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 39/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 40/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 41/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 42/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 43/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 44/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 45/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 46/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 47/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 48/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 49/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 50/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Loss : 12.0885753632
Accuracy :0.25


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : ATNT (40 persons - Uniform samples for each person : All faces are not recognized : PGM format) 
-------------------------------------------------------------------------

Images length : 263
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_19 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_19 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_13 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_20 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_20 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_14 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 40)                20520     
_________________________________________________________________
activation_21 (Activation)   (None, 40)                0         
=================================================================
Total params: 11,803,688
Trainable params: 11,803,688
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 236 samples, validate on 27 samples
Epoch 1/50
236/236 [==============================] - 6s - loss: 11.3055 - acc: 0.0297 - val_loss: 14.9212 - val_acc: 0.0000e+00
Epoch 2/50
236/236 [==============================] - 2s - loss: 15.0019 - acc: 0.0339 - val_loss: 14.9468 - val_acc: 0.0000e+00
Epoch 3/50
236/236 [==============================] - 2s - loss: 15.1252 - acc: 0.0424 - val_loss: 14.3590 - val_acc: 0.1111
Epoch 4/50
236/236 [==============================] - 2s - loss: 14.9108 - acc: 0.0551 - val_loss: 14.6074 - val_acc: 0.0741
Epoch 5/50
236/236 [==============================] - 2s - loss: 15.3833 - acc: 0.0254 - val_loss: 15.1709 - val_acc: 0.0000e+00
Epoch 6/50
236/236 [==============================] - 2s - loss: 15.3193 - acc: 0.0297 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 7/50
236/236 [==============================] - 2s - loss: 15.2915 - acc: 0.0466 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 8/50
236/236 [==============================] - 2s - loss: 14.9461 - acc: 0.0466 - val_loss: 14.1542 - val_acc: 0.0741
Epoch 9/50
236/236 [==============================] - 2s - loss: 15.0277 - acc: 0.0508 - val_loss: 13.5484 - val_acc: 0.1111
Epoch 10/50
236/236 [==============================] - 2s - loss: 15.1801 - acc: 0.0381 - val_loss: 14.2925 - val_acc: 0.0741
Epoch 11/50
236/236 [==============================] - 3s - loss: 15.5245 - acc: 0.0169 - val_loss: 14.8533 - val_acc: 0.0741
Epoch 12/50
236/236 [==============================] - 2s - loss: 15.4495 - acc: 0.0381 - val_loss: 14.1781 - val_acc: 0.1111
Epoch 13/50
236/236 [==============================] - 3s - loss: 15.2194 - acc: 0.0381 - val_loss: 14.9294 - val_acc: 0.0741
Epoch 14/50
236/236 [==============================] - 3s - loss: 15.6232 - acc: 0.0212 - val_loss: 15.5211 - val_acc: 0.0370
Epoch 15/50
236/236 [==============================] - 3s - loss: 15.1721 - acc: 0.0551 - val_loss: 15.4788 - val_acc: 0.0370
Epoch 16/50
236/236 [==============================] - 2s - loss: 15.3399 - acc: 0.0424 - val_loss: 14.9245 - val_acc: 0.0741
Epoch 17/50
236/236 [==============================] - 2s - loss: 15.3039 - acc: 0.0381 - val_loss: 13.7695 - val_acc: 0.1481
Epoch 18/50
236/236 [==============================] - 2s - loss: 14.9713 - acc: 0.0636 - val_loss: 14.2032 - val_acc: 0.1111
Epoch 19/50
236/236 [==============================] - 2s - loss: 15.0682 - acc: 0.0508 - val_loss: 14.6886 - val_acc: 0.0741
Epoch 20/50
236/236 [==============================] - 2s - loss: 15.2019 - acc: 0.0424 - val_loss: 14.6762 - val_acc: 0.0741
Epoch 21/50
236/236 [==============================] - 2s - loss: 14.9026 - acc: 0.0636 - val_loss: 13.9589 - val_acc: 0.0741
Epoch 22/50
236/236 [==============================] - 2s - loss: 14.9313 - acc: 0.0678 - val_loss: 14.2533 - val_acc: 0.0370
Epoch 23/50
236/236 [==============================] - 2s - loss: 15.0934 - acc: 0.0551 - val_loss: 13.8270 - val_acc: 0.1111
Epoch 24/50
236/236 [==============================] - 2s - loss: 15.0676 - acc: 0.0551 - val_loss: 13.7305 - val_acc: 0.1481
Epoch 25/50
236/236 [==============================] - 2s - loss: 15.0589 - acc: 0.0593 - val_loss: 14.1470 - val_acc: 0.0741
Epoch 26/50
236/236 [==============================] - 2s - loss: 15.0702 - acc: 0.0593 - val_loss: 14.3759 - val_acc: 0.0741
Epoch 27/50
236/236 [==============================] - 2s - loss: 15.0905 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 28/50
236/236 [==============================] - 2s - loss: 15.1270 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 29/50
236/236 [==============================] - 2s - loss: 15.0898 - acc: 0.0593 - val_loss: 14.6744 - val_acc: 0.0741
Epoch 30/50
236/236 [==============================] - 2s - loss: 15.1164 - acc: 0.0593 - val_loss: 14.3274 - val_acc: 0.1111
Epoch 31/50
236/236 [==============================] - 2s - loss: 15.1280 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 32/50
236/236 [==============================] - 2s - loss: 15.0809 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 33/50
236/236 [==============================] - 2s - loss: 15.0539 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 34/50
236/236 [==============================] - 2s - loss: 15.0290 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 35/50
236/236 [==============================] - 3s - loss: 15.0301 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 36/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 37/50
236/236 [==============================] - 2s - loss: 15.0519 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 38/50
236/236 [==============================] - 3s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 39/50
236/236 [==============================] - 4s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 40/50
236/236 [==============================] - 2s - loss: 15.0254 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 41/50
236/236 [==============================] - 2s - loss: 15.0772 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 42/50
236/236 [==============================] - 2s - loss: 15.0283 - acc: 0.0636 - val_loss: 14.6645 - val_acc: 0.0741
Epoch 43/50
236/236 [==============================] - 2s - loss: 15.1621 - acc: 0.0593 - val_loss: 14.9242 - val_acc: 0.0741
Epoch 44/50
236/236 [==============================] - 2s - loss: 15.1230 - acc: 0.0551 - val_loss: 14.3273 - val_acc: 0.1111
Epoch 45/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 46/50
236/236 [==============================] - 2s - loss: 15.0629 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 47/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 48/50
236/236 [==============================] - 2s - loss: 15.0254 - acc: 0.0678 - val_loss: 14.4264 - val_acc: 0.0741
Epoch 49/50
236/236 [==============================] - 2s - loss: 15.0578 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 50/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Loss : 14.3272008896
Accuracy :0.111111111939



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : LFW SMALL - GOOD  (50 persons - All having number of images > 10 - In JPEG format) 
-------------------------------------------------------------------------

model = Sequential()
model.add(Dense(128,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))


model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/10
526/526 [==============================] - 3s - loss: 4.3456 - acc: 0.0209 - val_loss: 5.6132 - val_acc: 0.0169
Epoch 2/10
526/526 [==============================] - 0s - loss: 4.2532 - acc: 0.0456 - val_loss: 4.8437 - val_acc: 0.0000e+00
Epoch 3/10
526/526 [==============================] - 0s - loss: 4.0466 - acc: 0.0361 - val_loss: 4.8265 - val_acc: 0.0339
Epoch 4/10
526/526 [==============================] - 0s - loss: 4.0122 - acc: 0.0456 - val_loss: 4.1759 - val_acc: 0.0508
Epoch 5/10
526/526 [==============================] - 0s - loss: 3.8102 - acc: 0.0684 - val_loss: 4.2375 - val_acc: 0.0169
Epoch 6/10
526/526 [==============================] - 0s - loss: 3.8019 - acc: 0.0627 - val_loss: 4.6711 - val_acc: 0.0000e+00
Epoch 7/10
526/526 [==============================] - 0s - loss: 3.7289 - acc: 0.0989 - val_loss: 4.5663 - val_acc: 0.0169
Epoch 8/10
526/526 [==============================] - 0s - loss: 3.7125 - acc: 0.0951 - val_loss: 4.0555 - val_acc: 0.0339
Epoch 9/10
526/526 [==============================] - 0s - loss: 3.5870 - acc: 0.1141 - val_loss: 4.1912 - val_acc: 0.0169
Epoch 10/10
526/526 [==============================] - 1s - loss: 3.4993 - acc: 0.1331 - val_loss: 4.0920 - val_acc: 0.0847
59/59 [==============================] - 0s     

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("Accuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/10
526/526 [==============================] - 4s - loss: 3.5000 - acc: 0.1084 - val_loss: 4.2573 - val_acc: 0.0508
Epoch 2/10
526/526 [==============================] - 1s - loss: 3.3488 - acc: 0.1578 - val_loss: 5.2315 - val_acc: 0.0000e+00
Epoch 3/10
526/526 [==============================] - 1s - loss: 3.6046 - acc: 0.1426 - val_loss: 4.1777 - val_acc: 0.0339
Epoch 4/10
526/526 [==============================] - 0s - loss: 3.3260 - acc: 0.1692 - val_loss: 3.7488 - val_acc: 0.0339
Epoch 5/10
526/526 [==============================] - 0s - loss: 3.1370 - acc: 0.1901 - val_loss: 5.5753 - val_acc: 0.0508
Epoch 6/10
526/526 [==============================] - 0s - loss: 3.7491 - acc: 0.1502 - val_loss: 4.3526 - val_acc: 0.0169
Epoch 7/10
526/526 [==============================] - 0s - loss: 3.1376 - acc: 0.2319 - val_loss: 4.1007 - val_acc: 0.0847
Epoch 8/10
526/526 [==============================] - 0s - loss: 3.0683 - acc: 0.2414 - val_loss: 4.9063 - val_acc: 0.0339
Epoch 9/10
526/526 [==============================] - 0s - loss: 3.1362 - acc: 0.2376 - val_loss: 4.6447 - val_acc: 0.0000e+00
Epoch 10/10
526/526 [==============================] - 0s - loss: 3.1748 - acc: 0.2129 - val_loss: 5.5766 - val_acc: 0.0339
32/59 [===============>..............] - ETA: 0sAccuracy : 0.0338983050847

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/50
526/526 [==============================] - 2s - loss: 3.4424 - acc: 0.1768 - val_loss: 3.7278 - val_acc: 0.0678
Epoch 2/50
526/526 [==============================] - 1s - loss: 2.8205 - acc: 0.2890 - val_loss: 5.3244 - val_acc: 0.0508
Epoch 3/50
526/526 [==============================] - 1s - loss: 3.2729 - acc: 0.2186 - val_loss: 4.7578 - val_acc: 0.0000e+00
Epoch 4/50
526/526 [==============================] - 0s - loss: 3.3322 - acc: 0.1844 - val_loss: 4.6859 - val_acc: 0.0000e+00
Epoch 5/50
526/526 [==============================] - 1s - loss: 3.1315 - acc: 0.2548 - val_loss: 4.2128 - val_acc: 0.0508
Epoch 6/50
526/526 [==============================] - 1s - loss: 3.0174 - acc: 0.2700 - val_loss: 4.4012 - val_acc: 0.0169
Epoch 7/50
526/526 [==============================] - 0s - loss: 2.6937 - acc: 0.3194 - val_loss: 3.8243 - val_acc: 0.1356
Epoch 8/50
526/526 [==============================] - 0s - loss: 2.4239 - acc: 0.4087 - val_loss: 3.9408 - val_acc: 0.0678
Epoch 9/50
526/526 [==============================] - 0s - loss: 2.4757 - acc: 0.3536 - val_loss: 4.2514 - val_acc: 0.0847
Epoch 10/50
526/526 [==============================] - 1s - loss: 2.5531 - acc: 0.3327 - val_loss: 6.1066 - val_acc: 0.0169
Epoch 11/50
526/526 [==============================] - 0s - loss: 3.4689 - acc: 0.2700 - val_loss: 3.7984 - val_acc: 0.0678
Epoch 12/50
526/526 [==============================] - 0s - loss: 2.6341 - acc: 0.3745 - val_loss: 5.0049 - val_acc: 0.0847
Epoch 13/50
526/526 [==============================] - 0s - loss: 2.7328 - acc: 0.3175 - val_loss: 3.9860 - val_acc: 0.0339
Epoch 14/50
526/526 [==============================] - 0s - loss: 2.3969 - acc: 0.3821 - val_loss: 4.2373 - val_acc: 0.0169
Epoch 15/50
526/526 [==============================] - 0s - loss: 2.3789 - acc: 0.3669 - val_loss: 4.4909 - val_acc: 0.0508
Epoch 16/50
526/526 [==============================] - 0s - loss: 2.4951 - acc: 0.3669 - val_loss: 3.7294 - val_acc: 0.1186
Epoch 17/50
526/526 [==============================] - 0s - loss: 2.2700 - acc: 0.4316 - val_loss: 4.1497 - val_acc: 0.0847
Epoch 18/50
526/526 [==============================] - 0s - loss: 2.2731 - acc: 0.4163 - val_loss: 3.7061 - val_acc: 0.1017
Epoch 19/50
526/526 [==============================] - 0s - loss: 2.1508 - acc: 0.4544 - val_loss: 4.1401 - val_acc: 0.1017
Epoch 20/50
526/526 [==============================] - 0s - loss: 2.2438 - acc: 0.4259 - val_loss: 3.9011 - val_acc: 0.1186
Epoch 21/50
526/526 [==============================] - 0s - loss: 2.0195 - acc: 0.4962 - val_loss: 4.3006 - val_acc: 0.1017
Epoch 22/50
526/526 [==============================] - 0s - loss: 2.1686 - acc: 0.4468 - val_loss: 4.4132 - val_acc: 0.1356
Epoch 23/50
526/526 [==============================] - 0s - loss: 2.1226 - acc: 0.4715 - val_loss: 4.3951 - val_acc: 0.1017
Epoch 24/50
526/526 [==============================] - 0s - loss: 2.0927 - acc: 0.4354 - val_loss: 4.1587 - val_acc: 0.0847
Epoch 25/50
526/526 [==============================] - 0s - loss: 2.0065 - acc: 0.4620 - val_loss: 4.1087 - val_acc: 0.0847
Epoch 26/50
526/526 [==============================] - 0s - loss: 1.9119 - acc: 0.5152 - val_loss: 3.9031 - val_acc: 0.1864
Epoch 27/50
526/526 [==============================] - 0s - loss: 2.0004 - acc: 0.5000 - val_loss: 3.7811 - val_acc: 0.1356
Epoch 28/50
526/526 [==============================] - 0s - loss: 1.8696 - acc: 0.5437 - val_loss: 4.1632 - val_acc: 0.1356
Epoch 29/50
526/526 [==============================] - 0s - loss: 1.9505 - acc: 0.4943 - val_loss: 4.4349 - val_acc: 0.0678
Epoch 30/50
526/526 [==============================] - 0s - loss: 1.9175 - acc: 0.5076 - val_loss: 4.0454 - val_acc: 0.0678
Epoch 31/50
526/526 [==============================] - 0s - loss: 1.8703 - acc: 0.5418 - val_loss: 5.8376 - val_acc: 0.0678
Epoch 32/50
526/526 [==============================] - 0s - loss: 2.1659 - acc: 0.5095 - val_loss: 4.3538 - val_acc: 0.1186
Epoch 33/50
526/526 [==============================] - 0s - loss: 2.0050 - acc: 0.5133 - val_loss: 4.3811 - val_acc: 0.0508
Epoch 34/50
526/526 [==============================] - 0s - loss: 1.8199 - acc: 0.5361 - val_loss: 4.3517 - val_acc: 0.0678
Epoch 35/50
526/526 [==============================] - 0s - loss: 1.9613 - acc: 0.5019 - val_loss: 4.0983 - val_acc: 0.0678
Epoch 36/50
526/526 [==============================] - 0s - loss: 1.7180 - acc: 0.5760 - val_loss: 4.3693 - val_acc: 0.0847
Epoch 37/50
526/526 [==============================] - 0s - loss: 1.7236 - acc: 0.5532 - val_loss: 3.9760 - val_acc: 0.1186
Epoch 38/50
526/526 [==============================] - 0s - loss: 1.6252 - acc: 0.5798 - val_loss: 4.0919 - val_acc: 0.1186
Epoch 39/50
526/526 [==============================] - 0s - loss: 1.5616 - acc: 0.5989 - val_loss: 4.6419 - val_acc: 0.0678
Epoch 40/50
526/526 [==============================] - 0s - loss: 1.7251 - acc: 0.5532 - val_loss: 3.9010 - val_acc: 0.0847
Epoch 41/50
526/526 [==============================] - 0s - loss: 1.6790 - acc: 0.5627 - val_loss: 3.5269 - val_acc: 0.0847
Epoch 42/50
526/526 [==============================] - 0s - loss: 1.4476 - acc: 0.6293 - val_loss: 4.0257 - val_acc: 0.1525
Epoch 43/50
526/526 [==============================] - 0s - loss: 1.4963 - acc: 0.6217 - val_loss: 5.3686 - val_acc: 0.0847
Epoch 44/50
526/526 [==============================] - 0s - loss: 1.8616 - acc: 0.5817 - val_loss: 3.5864 - val_acc: 0.1525
Epoch 45/50
526/526 [==============================] - 0s - loss: 1.3996 - acc: 0.6350 - val_loss: 4.0845 - val_acc: 0.1525
Epoch 46/50
526/526 [==============================] - 0s - loss: 1.4612 - acc: 0.6388 - val_loss: 3.6215 - val_acc: 0.1525
Epoch 47/50
526/526 [==============================] - 1s - loss: 1.2218 - acc: 0.6996 - val_loss: 3.7574 - val_acc: 0.1186
Epoch 48/50
526/526 [==============================] - 0s - loss: 1.2754 - acc: 0.6806 - val_loss: 3.9975 - val_acc: 0.1186
Epoch 49/50
526/526 [==============================] - 0s - loss: 1.3228 - acc: 0.6692 - val_loss: 4.2648 - val_acc: 0.1356
Epoch 50/50
526/526 [==============================] - 0s - loss: 1.2278 - acc: 0.7129 - val_loss: 3.8127 - val_acc: 0.1525
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.15254237326

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/100
526/526 [==============================] - 4s - loss: 1.2642 - acc: 0.6787 - val_loss: 5.9221 - val_acc: 0.0678
Epoch 2/100
526/526 [==============================] - 0s - loss: 1.6634 - acc: 0.6388 - val_loss: 3.9648 - val_acc: 0.1356
Epoch 3/100
526/526 [==============================] - 0s - loss: 1.3247 - acc: 0.6749 - val_loss: 3.7628 - val_acc: 0.1186
Epoch 4/100
526/526 [==============================] - 0s - loss: 1.1211 - acc: 0.7243 - val_loss: 3.7223 - val_acc: 0.1525
Epoch 5/100
526/526 [==============================] - 0s - loss: 1.0503 - acc: 0.7586 - val_loss: 4.5698 - val_acc: 0.1017
Epoch 6/100
526/526 [==============================] - 0s - loss: 1.2892 - acc: 0.6806 - val_loss: 4.6337 - val_acc: 0.1356
Epoch 7/100
526/526 [==============================] - 0s - loss: 1.3053 - acc: 0.6844 - val_loss: 3.7494 - val_acc: 0.1356
Epoch 8/100
526/526 [==============================] - 0s - loss: 1.0312 - acc: 0.7548 - val_loss: 3.9890 - val_acc: 0.1356
Epoch 9/100
526/526 [==============================] - 0s - loss: 1.0564 - acc: 0.7738 - val_loss: 3.8090 - val_acc: 0.1695
Epoch 10/100
526/526 [==============================] - 0s - loss: 1.2007 - acc: 0.6787 - val_loss: 3.9195 - val_acc: 0.1695
Epoch 11/100
526/526 [==============================] - 0s - loss: 1.0874 - acc: 0.7300 - val_loss: 3.6650 - val_acc: 0.2203
Epoch 12/100
526/526 [==============================] - 0s - loss: 0.9096 - acc: 0.7928 - val_loss: 3.6133 - val_acc: 0.2034
Epoch 13/100
526/526 [==============================] - 0s - loss: 0.9400 - acc: 0.7662 - val_loss: 4.1190 - val_acc: 0.1186
Epoch 14/100
526/526 [==============================] - 0s - loss: 1.0259 - acc: 0.7490 - val_loss: 4.1199 - val_acc: 0.1356
Epoch 15/100
526/526 [==============================] - 0s - loss: 1.2773 - acc: 0.7110 - val_loss: 3.8035 - val_acc: 0.1186
Epoch 16/100
526/526 [==============================] - 0s - loss: 0.9145 - acc: 0.8004 - val_loss: 3.9489 - val_acc: 0.1186
Epoch 17/100
526/526 [==============================] - 0s - loss: 0.8410 - acc: 0.8118 - val_loss: 3.5650 - val_acc: 0.2203
Epoch 18/100
526/526 [==============================] - 0s - loss: 0.8938 - acc: 0.7947 - val_loss: 4.1428 - val_acc: 0.1186
Epoch 19/100
526/526 [==============================] - 0s - loss: 1.0292 - acc: 0.7357 - val_loss: 3.7427 - val_acc: 0.1356
Epoch 20/100
526/526 [==============================] - 0s - loss: 0.9896 - acc: 0.7605 - val_loss: 3.8049 - val_acc: 0.1356
Epoch 21/100
526/526 [==============================] - 0s - loss: 0.9451 - acc: 0.7719 - val_loss: 3.3177 - val_acc: 0.2373
Epoch 22/100
526/526 [==============================] - 0s - loss: 0.7789 - acc: 0.8536 - val_loss: 3.3956 - val_acc: 0.2034
Epoch 23/100
526/526 [==============================] - 0s - loss: 0.7694 - acc: 0.8289 - val_loss: 4.4055 - val_acc: 0.1186
Epoch 24/100
526/526 [==============================] - 1s - loss: 0.8495 - acc: 0.8137 - val_loss: 4.3330 - val_acc: 0.1186
Epoch 25/100
526/526 [==============================] - 0s - loss: 0.8671 - acc: 0.8099 - val_loss: 3.3240 - val_acc: 0.2881
Epoch 26/100
526/526 [==============================] - 0s - loss: 0.7455 - acc: 0.8460 - val_loss: 4.3340 - val_acc: 0.1695
Epoch 27/100
526/526 [==============================] - 0s - loss: 1.1851 - acc: 0.7357 - val_loss: 3.6501 - val_acc: 0.1356
Epoch 28/100
526/526 [==============================] - 0s - loss: 0.7901 - acc: 0.8213 - val_loss: 4.6090 - val_acc: 0.1356
Epoch 29/100
526/526 [==============================] - 0s - loss: 0.8194 - acc: 0.8270 - val_loss: 3.6600 - val_acc: 0.2203
Epoch 30/100
526/526 [==============================] - 0s - loss: 0.6740 - acc: 0.8783 - val_loss: 3.8488 - val_acc: 0.2203
Epoch 31/100
526/526 [==============================] - 0s - loss: 0.6892 - acc: 0.8764 - val_loss: 3.7510 - val_acc: 0.2034
Epoch 32/100
526/526 [==============================] - 0s - loss: 0.7083 - acc: 0.8783 - val_loss: 3.2177 - val_acc: 0.2203
Epoch 33/100
526/526 [==============================] - 0s - loss: 0.6182 - acc: 0.8821 - val_loss: 3.5354 - val_acc: 0.1864
Epoch 34/100
526/526 [==============================] - 0s - loss: 0.6110 - acc: 0.8840 - val_loss: 3.8481 - val_acc: 0.2373
Epoch 35/100
526/526 [==============================] - 0s - loss: 0.6263 - acc: 0.8783 - val_loss: 5.1863 - val_acc: 0.0678
Epoch 36/100
526/526 [==============================] - 0s - loss: 0.9329 - acc: 0.7700 - val_loss: 4.7938 - val_acc: 0.1356
Epoch 37/100
526/526 [==============================] - 0s - loss: 0.8695 - acc: 0.8042 - val_loss: 3.5721 - val_acc: 0.2373
Epoch 38/100
526/526 [==============================] - 0s - loss: 0.6112 - acc: 0.8878 - val_loss: 4.0780 - val_acc: 0.1186
Epoch 39/100
526/526 [==============================] - 0s - loss: 0.6509 - acc: 0.8764 - val_loss: 3.5682 - val_acc: 0.2542
Epoch 40/100
526/526 [==============================] - 0s - loss: 0.5217 - acc: 0.9144 - val_loss: 3.6885 - val_acc: 0.2034
Epoch 41/100
526/526 [==============================] - 0s - loss: 0.5392 - acc: 0.8954 - val_loss: 3.4325 - val_acc: 0.2373
Epoch 42/100
526/526 [==============================] - 0s - loss: 0.5110 - acc: 0.9125 - val_loss: 4.1316 - val_acc: 0.1864
Epoch 43/100
526/526 [==============================] - 0s - loss: 0.6322 - acc: 0.8840 - val_loss: 3.7461 - val_acc: 0.2034
Epoch 44/100
526/526 [==============================] - 0s - loss: 0.4976 - acc: 0.9297 - val_loss: 3.7588 - val_acc: 0.2203
Epoch 45/100
526/526 [==============================] - 0s - loss: 0.4766 - acc: 0.9278 - val_loss: 4.0636 - val_acc: 0.1695
Epoch 46/100
526/526 [==============================] - 0s - loss: 0.7503 - acc: 0.8631 - val_loss: 3.6113 - val_acc: 0.2373
Epoch 47/100
526/526 [==============================] - 0s - loss: 0.4998 - acc: 0.9354 - val_loss: 3.7159 - val_acc: 0.1695
Epoch 48/100
526/526 [==============================] - 0s - loss: 0.4977 - acc: 0.9221 - val_loss: 3.7000 - val_acc: 0.2034
Epoch 49/100
526/526 [==============================] - 0s - loss: 0.4717 - acc: 0.9316 - val_loss: 4.7497 - val_acc: 0.1525
Epoch 50/100
526/526 [==============================] - 1s - loss: 0.6078 - acc: 0.8726 - val_loss: 3.2380 - val_acc: 0.2712
Epoch 51/100
526/526 [==============================] - 0s - loss: 0.4017 - acc: 0.9468 - val_loss: 3.1883 - val_acc: 0.2712
Epoch 52/100
526/526 [==============================] - 0s - loss: 0.4068 - acc: 0.9544 - val_loss: 3.2930 - val_acc: 0.2712
Epoch 53/100
526/526 [==============================] - 0s - loss: 0.3921 - acc: 0.9544 - val_loss: 3.3287 - val_acc: 0.2712
Epoch 54/100
526/526 [==============================] - 0s - loss: 0.3896 - acc: 0.9563 - val_loss: 4.6032 - val_acc: 0.2203
Epoch 55/100
526/526 [==============================] - 0s - loss: 1.0231 - acc: 0.8156 - val_loss: 3.5519 - val_acc: 0.2203
Epoch 56/100
526/526 [==============================] - 0s - loss: 0.4653 - acc: 0.9487 - val_loss: 3.5094 - val_acc: 0.2881
Epoch 57/100
526/526 [==============================] - 0s - loss: 0.3898 - acc: 0.9582 - val_loss: 3.9603 - val_acc: 0.1356
Epoch 58/100
526/526 [==============================] - 0s - loss: 0.6231 - acc: 0.8650 - val_loss: 3.6630 - val_acc: 0.2373
Epoch 59/100
526/526 [==============================] - 0s - loss: 0.3981 - acc: 0.9563 - val_loss: 3.1190 - val_acc: 0.3559
Epoch 60/100
526/526 [==============================] - 0s - loss: 0.3432 - acc: 0.9715 - val_loss: 3.8199 - val_acc: 0.1186
Epoch 61/100
526/526 [==============================] - 0s - loss: 0.4114 - acc: 0.9335 - val_loss: 4.1469 - val_acc: 0.1356
Epoch 62/100
526/526 [==============================] - 0s - loss: 0.4179 - acc: 0.9297 - val_loss: 3.4904 - val_acc: 0.2712
Epoch 63/100
526/526 [==============================] - 0s - loss: 0.3499 - acc: 0.9582 - val_loss: 3.3167 - val_acc: 0.2712
Epoch 64/100
526/526 [==============================] - 0s - loss: 0.3356 - acc: 0.9715 - val_loss: 3.2701 - val_acc: 0.2373
Epoch 65/100
526/526 [==============================] - 0s - loss: 0.3493 - acc: 0.9620 - val_loss: 3.2205 - val_acc: 0.3051
Epoch 66/100
526/526 [==============================] - 1s - loss: 0.3368 - acc: 0.9658 - val_loss: 3.2189 - val_acc: 0.2542
Epoch 67/100
526/526 [==============================] - 0s - loss: 0.3201 - acc: 0.9696 - val_loss: 3.2326 - val_acc: 0.3220
Epoch 68/100
526/526 [==============================] - 1s - loss: 0.2965 - acc: 0.9829 - val_loss: 3.4450 - val_acc: 0.2203
Epoch 69/100
526/526 [==============================] - 0s - loss: 0.3275 - acc: 0.9544 - val_loss: 4.0909 - val_acc: 0.1186
Epoch 70/100
526/526 [==============================] - 0s - loss: 0.4315 - acc: 0.9259 - val_loss: 3.5124 - val_acc: 0.2881
Epoch 71/100
526/526 [==============================] - 0s - loss: 0.3270 - acc: 0.9525 - val_loss: 3.8139 - val_acc: 0.1695
Epoch 72/100
526/526 [==============================] - 1s - loss: 0.3707 - acc: 0.9506 - val_loss: 3.7695 - val_acc: 0.1864
Epoch 73/100
526/526 [==============================] - 0s - loss: 0.3741 - acc: 0.9430 - val_loss: 3.2835 - val_acc: 0.2712
Epoch 74/100
526/526 [==============================] - 0s - loss: 0.2634 - acc: 0.9886 - val_loss: 3.1591 - val_acc: 0.3051
Epoch 75/100
526/526 [==============================] - 0s - loss: 0.2759 - acc: 0.9772 - val_loss: 3.2590 - val_acc: 0.2373
Epoch 76/100
526/526 [==============================] - 0s - loss: 0.2763 - acc: 0.9810 - val_loss: 3.1903 - val_acc: 0.3051
Epoch 77/100
526/526 [==============================] - 0s - loss: 0.2510 - acc: 0.9905 - val_loss: 3.3822 - val_acc: 0.2542
Epoch 78/100
526/526 [==============================] - 1s - loss: 0.3054 - acc: 0.9772 - val_loss: 3.4774 - val_acc: 0.1864
Epoch 79/100
526/526 [==============================] - 1s - loss: 0.3590 - acc: 0.9506 - val_loss: 3.4670 - val_acc: 0.2881
Epoch 80/100
526/526 [==============================] - 1s - loss: 0.2591 - acc: 0.9791 - val_loss: 3.5594 - val_acc: 0.2542
Epoch 81/100
526/526 [==============================] - 1s - loss: 0.3735 - acc: 0.9392 - val_loss: 3.3879 - val_acc: 0.2542
Epoch 82/100
526/526 [==============================] - 1s - loss: 0.2619 - acc: 0.9791 - val_loss: 3.4444 - val_acc: 0.2712
Epoch 83/100
526/526 [==============================] - 1s - loss: 0.2675 - acc: 0.9734 - val_loss: 3.8936 - val_acc: 0.2373
Epoch 84/100
526/526 [==============================] - 0s - loss: 0.3561 - acc: 0.9563 - val_loss: 3.3732 - val_acc: 0.3051
Epoch 85/100
526/526 [==============================] - 0s - loss: 0.2247 - acc: 0.9886 - val_loss: 3.2430 - val_acc: 0.2373
Epoch 86/100
526/526 [==============================] - 0s - loss: 0.2286 - acc: 0.9924 - val_loss: 3.1362 - val_acc: 0.3559
Epoch 87/100
526/526 [==============================] - 0s - loss: 0.2229 - acc: 0.9943 - val_loss: 3.3249 - val_acc: 0.3051
Epoch 88/100
526/526 [==============================] - 0s - loss: 0.2209 - acc: 0.9867 - val_loss: 3.6700 - val_acc: 0.2542
Epoch 89/100
526/526 [==============================] - 0s - loss: 0.2775 - acc: 0.9696 - val_loss: 3.6555 - val_acc: 0.2203
Epoch 90/100
526/526 [==============================] - 1s - loss: 0.2325 - acc: 0.9867 - val_loss: 3.4558 - val_acc: 0.2542
Epoch 91/100
526/526 [==============================] - 0s - loss: 0.2255 - acc: 0.9810 - val_loss: 3.3554 - val_acc: 0.2881
Epoch 92/100
526/526 [==============================] - 0s - loss: 0.2076 - acc: 0.9905 - val_loss: 3.2934 - val_acc: 0.3051
Epoch 93/100
526/526 [==============================] - 0s - loss: 0.1981 - acc: 0.9962 - val_loss: 3.3076 - val_acc: 0.2881
Epoch 94/100
526/526 [==============================] - 0s - loss: 0.1902 - acc: 0.9981 - val_loss: 3.7088 - val_acc: 0.1695
Epoch 95/100
526/526 [==============================] - 0s - loss: 0.2231 - acc: 0.9829 - val_loss: 3.3317 - val_acc: 0.2712
Epoch 96/100
526/526 [==============================] - 0s - loss: 0.2023 - acc: 0.9905 - val_loss: 3.2898 - val_acc: 0.3051
Epoch 97/100
526/526 [==============================] - 0s - loss: 0.1956 - acc: 0.9943 - val_loss: 3.4920 - val_acc: 0.2203
Epoch 98/100
526/526 [==============================] - 0s - loss: 0.1956 - acc: 0.9962 - val_loss: 3.5599 - val_acc: 0.2881
Epoch 99/100
526/526 [==============================] - 0s - loss: 0.2111 - acc: 0.9829 - val_loss: 3.4665 - val_acc: 0.2542
Epoch 100/100
526/526 [==============================] - 0s - loss: 0.1960 - acc: 0.9943 - val_loss: 3.2468 - val_acc: 0.2881
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.288135594231



model = Sequential()
model.add(Dense(128,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('elu'))



model.compile(loss='sparse_categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=128, nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/100
526/526 [==============================] - 18s - loss: 11.2362 - acc: 0.0152 - val_loss: 12.4327 - val_acc: 0.0339
Epoch 2/100
526/526 [==============================] - 0s - loss: 11.0312 - acc: 0.0171 - val_loss: 12.2721 - val_acc: 0.0339
Epoch 3/100
526/526 [==============================] - 0s - loss: 10.6419 - acc: 0.0152 - val_loss: 12.2676 - val_acc: 0.0339
Epoch 4/100
526/526 [==============================] - 0s - loss: 10.8063 - acc: 0.0152 - val_loss: 12.5090 - val_acc: 0.0339
Epoch 5/100
526/526 [==============================] - 0s - loss: 10.7496 - acc: 0.0152 - val_loss: 12.2720 - val_acc: 0.0339
Epoch 6/100
526/526 [==============================] - 0s - loss: 10.6425 - acc: 0.0152 - val_loss: 12.2726 - val_acc: 0.0339
Epoch 7/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2729 - val_acc: 0.0339
Epoch 8/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 9/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 10/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 11/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 12/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 13/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 14/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 15/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 16/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 17/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 18/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 19/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 20/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 21/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 22/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 23/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 24/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 25/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 26/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 27/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 28/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 29/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 30/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 31/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 32/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 33/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 34/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 35/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 36/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 37/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 38/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 39/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 40/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 41/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 42/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 43/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 44/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 45/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 46/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 47/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 48/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 49/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 50/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 51/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 52/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 53/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 54/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 55/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 56/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 57/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 58/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 59/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 60/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 61/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 62/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 63/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 64/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 65/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 66/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 67/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 68/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 69/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 70/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 71/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 72/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 73/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 74/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 75/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 76/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 77/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 78/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 79/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 80/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 81/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 82/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 83/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 84/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 85/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 86/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 87/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 88/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 89/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 90/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 91/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 92/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 93/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 94/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 95/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 96/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 97/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 98/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 99/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 100/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.0338983053373



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : Yale Database 
-------------------------------------------------------------------------


Images length : 153
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_3 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 4s - loss: 7.7289 - acc: 0.0657 - val_loss: 13.9477 - val_acc: 0.1250
Epoch 2/50
137/137 [==============================] - 1s - loss: 13.7305 - acc: 0.1095 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 3/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 4/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 5/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 6/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 7/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 8/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 9/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 10/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 11/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 12/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 13/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 14/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 15/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 16/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 17/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 18/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 19/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 20/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 21/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 22/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 23/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 24/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 25/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 26/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 27/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 28/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 29/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 30/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 31/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 32/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 33/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 34/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 35/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 36/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 37/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 38/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 39/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 40/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 41/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 42/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 43/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 44/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 45/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 46/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 47/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 48/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 49/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 50/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Loss : 15.1107158661
Accuracy :0.0625





model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_4 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_4 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_6 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 1s - loss: 3.6166 - acc: 0.1168 - val_loss: 3.5510 - val_acc: 0.0000e+00
Epoch 2/50
137/137 [==============================] - 0s - loss: 4.4580 - acc: 0.1095 - val_loss: 3.7502 - val_acc: 0.1250
Epoch 3/50
137/137 [==============================] - 1s - loss: 4.8089 - acc: 0.1022 - val_loss: 4.2269 - val_acc: 0.2500
Epoch 4/50
137/137 [==============================] - 1s - loss: 3.9124 - acc: 0.2482 - val_loss: 4.2498 - val_acc: 0.1250
Epoch 5/50
137/137 [==============================] - 1s - loss: 3.4421 - acc: 0.1387 - val_loss: 3.3991 - val_acc: 0.1250
Epoch 6/50
137/137 [==============================] - 0s - loss: 2.1582 - acc: 0.3577 - val_loss: 2.8955 - val_acc: 0.1875
Epoch 7/50
137/137 [==============================] - 0s - loss: 2.0231 - acc: 0.3869 - val_loss: 2.0608 - val_acc: 0.3125
Epoch 8/50
137/137 [==============================] - 1s - loss: 1.8043 - acc: 0.4380 - val_loss: 2.2195 - val_acc: 0.3125
Epoch 9/50
137/137 [==============================] - 1s - loss: 2.0236 - acc: 0.3577 - val_loss: 3.4846 - val_acc: 0.3125
Epoch 10/50
137/137 [==============================] - 1s - loss: 1.9147 - acc: 0.4891 - val_loss: 2.6191 - val_acc: 0.0625
Epoch 11/50
137/137 [==============================] - 1s - loss: 1.4545 - acc: 0.5693 - val_loss: 1.2435 - val_acc: 0.6250
Epoch 12/50
137/137 [==============================] - 1s - loss: 1.0327 - acc: 0.7226 - val_loss: 2.0393 - val_acc: 0.3125
Epoch 13/50
137/137 [==============================] - 1s - loss: 1.6211 - acc: 0.5693 - val_loss: 1.7130 - val_acc: 0.5625
Epoch 14/50
137/137 [==============================] - 1s - loss: 1.0391 - acc: 0.7956 - val_loss: 2.1075 - val_acc: 0.3125
Epoch 15/50
137/137 [==============================] - 1s - loss: 1.3576 - acc: 0.6715 - val_loss: 1.5027 - val_acc: 0.3750
Epoch 16/50
137/137 [==============================] - 0s - loss: 1.1084 - acc: 0.6350 - val_loss: 0.8839 - val_acc: 0.7500
Epoch 17/50
137/137 [==============================] - 1s - loss: 0.5557 - acc: 0.9270 - val_loss: 0.7150 - val_acc: 0.8750
Epoch 18/50
137/137 [==============================] - 0s - loss: 0.4581 - acc: 0.9489 - val_loss: 0.8322 - val_acc: 0.8125
Epoch 19/50
137/137 [==============================] - 0s - loss: 0.4219 - acc: 0.9343 - val_loss: 0.7190 - val_acc: 0.6875
Epoch 20/50
137/137 [==============================] - 0s - loss: 0.6266 - acc: 0.8102 - val_loss: 1.0861 - val_acc: 0.5625
Epoch 21/50
137/137 [==============================] - 0s - loss: 0.6668 - acc: 0.8102 - val_loss: 0.5563 - val_acc: 0.8750
Epoch 22/50
137/137 [==============================] - 0s - loss: 0.2748 - acc: 0.9708 - val_loss: 0.6295 - val_acc: 0.8125
Epoch 23/50
137/137 [==============================] - 0s - loss: 0.2975 - acc: 0.9489 - val_loss: 0.5851 - val_acc: 0.8750
Epoch 24/50
137/137 [==============================] - 0s - loss: 0.2714 - acc: 0.9635 - val_loss: 0.4570 - val_acc: 0.9375
Epoch 25/50
137/137 [==============================] - 0s - loss: 0.2496 - acc: 0.9562 - val_loss: 0.6448 - val_acc: 0.8125
Epoch 26/50
137/137 [==============================] - 0s - loss: 0.2672 - acc: 0.9635 - val_loss: 0.7846 - val_acc: 0.6875
Epoch 27/50
137/137 [==============================] - 0s - loss: 0.3723 - acc: 0.9270 - val_loss: 0.4334 - val_acc: 0.9375
Epoch 28/50
137/137 [==============================] - 0s - loss: 0.1832 - acc: 0.9927 - val_loss: 1.1565 - val_acc: 0.5000
Epoch 29/50
137/137 [==============================] - 0s - loss: 0.5718 - acc: 0.8613 - val_loss: 0.4454 - val_acc: 0.9375
Epoch 30/50
137/137 [==============================] - 0s - loss: 0.1626 - acc: 1.0000 - val_loss: 0.4393 - val_acc: 0.8750
Epoch 31/50
137/137 [==============================] - 0s - loss: 0.1486 - acc: 1.0000 - val_loss: 0.5542 - val_acc: 0.6875
Epoch 32/50
137/137 [==============================] - 0s - loss: 0.1745 - acc: 0.9781 - val_loss: 0.3715 - val_acc: 0.9375
Epoch 33/50
137/137 [==============================] - 0s - loss: 0.1233 - acc: 1.0000 - val_loss: 0.3413 - val_acc: 0.9375
Epoch 34/50
137/137 [==============================] - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 0.3138 - val_acc: 1.0000
Epoch 35/50
137/137 [==============================] - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 0.4057 - val_acc: 0.9375
Epoch 36/50
137/137 [==============================] - 0s - loss: 0.1192 - acc: 1.0000 - val_loss: 0.3261 - val_acc: 0.9375
Epoch 37/50
137/137 [==============================] - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.3013 - val_acc: 0.9375
Epoch 38/50
137/137 [==============================] - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 0.3629 - val_acc: 0.8750
Epoch 39/50
137/137 [==============================] - 0s - loss: 0.0988 - acc: 1.0000 - val_loss: 0.3096 - val_acc: 0.9375
Epoch 40/50
137/137 [==============================] - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 0.9375
Epoch 41/50
137/137 [==============================] - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9375
Epoch 42/50
137/137 [==============================] - 0s - loss: 0.1111 - acc: 0.9927 - val_loss: 0.3351 - val_acc: 0.8750
Epoch 43/50
137/137 [==============================] - 0s - loss: 0.0799 - acc: 1.0000 - val_loss: 0.2785 - val_acc: 1.0000
Epoch 44/50
137/137 [==============================] - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.2520 - val_acc: 0.9375
Epoch 45/50
137/137 [==============================] - 0s - loss: 0.0727 - acc: 1.0000 - val_loss: 0.2668 - val_acc: 0.9375
Epoch 46/50
137/137 [==============================] - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.2915 - val_acc: 0.9375
Epoch 47/50
137/137 [==============================] - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.2624 - val_acc: 0.9375
Epoch 48/50
137/137 [==============================] - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.2937 - val_acc: 0.8750
Epoch 49/50
137/137 [==============================] - 0s - loss: 0.0649 - acc: 1.0000 - val_loss: 0.2584 - val_acc: 0.9375
Epoch 50/50
137/137 [==============================] - 1s - loss: 0.0625 - acc: 1.0000 - val_loss: 0.2456 - val_acc: 1.0000
Loss : 0.245639264584
Accuracy :1.0




 model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('relu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('relu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_7 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_7 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_8 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_9 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 1s - loss: 2.8315 - acc: 0.0803 - val_loss: 2.9226 - val_acc: 0.0000e+00
Epoch 2/50
137/137 [==============================] - 0s - loss: 2.8637 - acc: 0.1095 - val_loss: 2.9666 - val_acc: 0.1250
Epoch 3/50
137/137 [==============================] - 0s - loss: 2.5696 - acc: 0.2044 - val_loss: 2.8864 - val_acc: 0.1875
Epoch 4/50
137/137 [==============================] - 0s - loss: 2.2339 - acc: 0.3504 - val_loss: 2.8115 - val_acc: 0.1250
Epoch 5/50
137/137 [==============================] - 0s - loss: 2.4098 - acc: 0.2482 - val_loss: 2.9492 - val_acc: 0.0000e+00
Epoch 6/50
137/137 [==============================] - 0s - loss: 2.1558 - acc: 0.4161 - val_loss: 2.2735 - val_acc: 0.1250
Epoch 7/50
137/137 [==============================] - 0s - loss: 2.0459 - acc: 0.3504 - val_loss: 2.1732 - val_acc: 0.1250
Epoch 8/50
137/137 [==============================] - 0s - loss: 1.9670 - acc: 0.3504 - val_loss: 3.2072 - val_acc: 0.1250
Epoch 9/50
137/137 [==============================] - 0s - loss: 2.3194 - acc: 0.1971 - val_loss: 2.8379 - val_acc: 0.1250
Epoch 10/50
137/137 [==============================] - 0s - loss: 2.3258 - acc: 0.3358 - val_loss: 2.9615 - val_acc: 0.1250
Epoch 11/50
137/137 [==============================] - 0s - loss: 2.1722 - acc: 0.3577 - val_loss: 1.9016 - val_acc: 0.3750
Epoch 12/50
137/137 [==============================] - 0s - loss: 1.5235 - acc: 0.6496 - val_loss: 2.8282 - val_acc: 0.0000e+00
Epoch 13/50
137/137 [==============================] - 0s - loss: 1.6758 - acc: 0.5109 - val_loss: 1.7275 - val_acc: 0.2500
Epoch 14/50
137/137 [==============================] - 0s - loss: 1.3545 - acc: 0.6204 - val_loss: 2.1503 - val_acc: 0.1875
Epoch 15/50
137/137 [==============================] - 0s - loss: 1.5112 - acc: 0.5547 - val_loss: 1.6235 - val_acc: 0.3125
Epoch 16/50
137/137 [==============================] - 0s - loss: 1.2694 - acc: 0.6058 - val_loss: 2.2344 - val_acc: 0.1875
Epoch 17/50
137/137 [==============================] - 0s - loss: 1.5145 - acc: 0.5182 - val_loss: 1.5460 - val_acc: 0.3750
Epoch 18/50
137/137 [==============================] - 0s - loss: 1.0004 - acc: 0.8029 - val_loss: 1.3050 - val_acc: 0.5625
Epoch 19/50
137/137 [==============================] - 0s - loss: 0.9623 - acc: 0.7737 - val_loss: 1.3764 - val_acc: 0.6250
Epoch 20/50
137/137 [==============================] - 0s - loss: 1.2788 - acc: 0.6131 - val_loss: 1.9077 - val_acc: 0.3125
Epoch 21/50
137/137 [==============================] - 0s - loss: 1.1812 - acc: 0.6934 - val_loss: 1.2033 - val_acc: 0.7500
Epoch 22/50
137/137 [==============================] - 0s - loss: 1.0076 - acc: 0.7153 - val_loss: 0.8051 - val_acc: 0.8125
Epoch 23/50
137/137 [==============================] - 0s - loss: 0.5972 - acc: 0.9343 - val_loss: 0.8392 - val_acc: 0.8750
Epoch 24/50
137/137 [==============================] - 0s - loss: 0.5300 - acc: 0.9489 - val_loss: 1.2054 - val_acc: 0.5625
Epoch 25/50
137/137 [==============================] - 0s - loss: 0.6297 - acc: 0.8467 - val_loss: 0.7999 - val_acc: 0.8750
Epoch 26/50
137/137 [==============================] - 0s - loss: 0.5589 - acc: 0.9270 - val_loss: 0.8576 - val_acc: 0.7500
Epoch 27/50
137/137 [==============================] - 0s - loss: 0.4624 - acc: 0.9489 - val_loss: 0.9236 - val_acc: 0.6875
Epoch 28/50
137/137 [==============================] - 0s - loss: 0.4583 - acc: 0.9343 - val_loss: 0.7905 - val_acc: 0.8125
Epoch 29/50
137/137 [==============================] - 1s - loss: 0.4781 - acc: 0.8978 - val_loss: 0.6963 - val_acc: 0.8750
Epoch 30/50
137/137 [==============================] - 1s - loss: 0.3448 - acc: 0.9708 - val_loss: 0.9026 - val_acc: 0.7500
Epoch 31/50
137/137 [==============================] - 0s - loss: 0.7145 - acc: 0.8321 - val_loss: 0.6709 - val_acc: 0.8125
Epoch 32/50
137/137 [==============================] - 0s - loss: 0.3211 - acc: 0.9708 - val_loss: 0.5991 - val_acc: 0.8750
Epoch 33/50
137/137 [==============================] - 0s - loss: 0.3169 - acc: 0.9635 - val_loss: 0.5769 - val_acc: 0.8750
Epoch 34/50
137/137 [==============================] - 1s - loss: 0.2670 - acc: 0.9927 - val_loss: 0.7951 - val_acc: 0.6875
Epoch 35/50
137/137 [==============================] - 0s - loss: 0.2851 - acc: 0.9562 - val_loss: 0.4147 - val_acc: 1.0000
Epoch 36/50
137/137 [==============================] - 1s - loss: 0.2328 - acc: 1.0000 - val_loss: 0.4733 - val_acc: 0.9375
Epoch 37/50
137/137 [==============================] - 1s - loss: 0.2307 - acc: 0.9927 - val_loss: 0.8395 - val_acc: 0.5625
Epoch 38/50
137/137 [==============================] - 1s - loss: 0.2635 - acc: 0.9635 - val_loss: 0.3936 - val_acc: 1.0000
Epoch 39/50
137/137 [==============================] - 1s - loss: 0.2043 - acc: 0.9927 - val_loss: 0.4135 - val_acc: 0.9375
Epoch 40/50
137/137 [==============================] - 0s - loss: 0.1996 - acc: 1.0000 - val_loss: 0.4431 - val_acc: 0.9375
Epoch 41/50
137/137 [==============================] - 0s - loss: 0.2150 - acc: 0.9927 - val_loss: 0.4614 - val_acc: 0.9375
Epoch 42/50
137/137 [==============================] - 0s - loss: 0.2261 - acc: 0.9854 - val_loss: 0.3763 - val_acc: 0.9375
Epoch 43/50
137/137 [==============================] - 0s - loss: 0.1635 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 0.9375
Epoch 44/50
137/137 [==============================] - 0s - loss: 0.1851 - acc: 0.9854 - val_loss: 0.3637 - val_acc: 0.9375
Epoch 45/50
137/137 [==============================] - 0s - loss: 0.1506 - acc: 1.0000 - val_loss: 0.3916 - val_acc: 0.9375
Epoch 46/50
137/137 [==============================] - 0s - loss: 0.1523 - acc: 1.0000 - val_loss: 0.4020 - val_acc: 0.8750
Epoch 47/50
137/137 [==============================] - 0s - loss: 0.1468 - acc: 1.0000 - val_loss: 0.3758 - val_acc: 0.9375
Epoch 48/50
137/137 [==============================] - 0s - loss: 0.1494 - acc: 0.9927 - val_loss: 0.4207 - val_acc: 0.9375
Epoch 49/50
137/137 [==============================] - 0s - loss: 0.1443 - acc: 1.0000 - val_loss: 0.4195 - val_acc: 0.8750
Epoch 50/50
137/137 [==============================] - 0s - loss: 0.1370 - acc: 1.0000 - val_loss: 0.3187 - val_acc: 0.9375
Loss : 0.318680644035
Accuracy :0.9375





 model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_10 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_10 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_11 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_11 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_12 (Dense)             (None, 14)                7182      
_________________________________________________________________
activation_12 (Activation)   (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 8.5353 - acc: 0.0219 - val_loss: 13.0127 - val_acc: 0.1250
Epoch 2/50
137/137 [==============================] - 2s - loss: 13.4688 - acc: 0.0438 - val_loss: 14.0262 - val_acc: 0.1250
Epoch 3/50
137/137 [==============================] - 1s - loss: 15.0007 - acc: 0.0657 - val_loss: 13.1548 - val_acc: 0.1250
Epoch 4/50
137/137 [==============================] - 1s - loss: 14.0010 - acc: 0.1314 - val_loss: 13.5400 - val_acc: 0.1250
Epoch 5/50
137/137 [==============================] - 1s - loss: 14.1228 - acc: 0.1241 - val_loss: 13.8932 - val_acc: 0.1250
Epoch 6/50
137/137 [==============================] - 1s - loss: 14.0850 - acc: 0.1168 - val_loss: 13.5246 - val_acc: 0.1250
Epoch 7/50
137/137 [==============================] - 1s - loss: 14.1073 - acc: 0.1095 - val_loss: 13.8421 - val_acc: 0.1250
Epoch 8/50
137/137 [==============================] - 2s - loss: 14.1790 - acc: 0.1095 - val_loss: 13.6873 - val_acc: 0.1250
Epoch 9/50
137/137 [==============================] - 2s - loss: 14.0976 - acc: 0.1241 - val_loss: 13.1766 - val_acc: 0.1250
Epoch 10/50
137/137 [==============================] - 1s - loss: 14.0705 - acc: 0.1241 - val_loss: 13.0960 - val_acc: 0.1875
Epoch 11/50
137/137 [==============================] - 1s - loss: 14.0775 - acc: 0.1241 - val_loss: 14.5963 - val_acc: 0.0625
Epoch 12/50
137/137 [==============================] - 1s - loss: 14.3716 - acc: 0.0949 - val_loss: 13.0969 - val_acc: 0.1875
Epoch 13/50
137/137 [==============================] - 1s - loss: 14.0004 - acc: 0.1314 - val_loss: 13.0960 - val_acc: 0.1875
Epoch 14/50
137/137 [==============================] - 1s - loss: 14.0004 - acc: 0.1314 - val_loss: 13.9764 - val_acc: 0.1250
Epoch 15/50
137/137 [==============================] - 1s - loss: 14.0909 - acc: 0.1168 - val_loss: 13.0960 - val_acc: 0.1875
Epoch 16/50
137/137 [==============================] - 1s - loss: 14.0004 - acc: 0.1314 - val_loss: 13.3145 - val_acc: 0.1250
Epoch 17/50
137/137 [==============================] - 1s - loss: 14.0647 - acc: 0.1241 - val_loss: 13.0960 - val_acc: 0.1875
Epoch 18/50
137/137 [==============================] - 1s - loss: 14.3828 - acc: 0.0949 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 19/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 20/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 21/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 22/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 23/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 24/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 25/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 26/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 27/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 28/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 29/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 30/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 31/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 32/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 33/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 34/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 35/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 36/50
137/137 [==============================] - 2s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 37/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 38/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 39/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 40/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 41/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 42/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 43/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 44/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 45/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 46/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 47/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 48/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 49/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Epoch 50/50
137/137 [==============================] - 1s - loss: 15.0592 - acc: 0.0657 - val_loss: 14.1033 - val_acc: 0.1250
Loss : 14.1033353806
Accuracy :0.125









Images length : 153
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_3 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 7s - loss: 8.0890 - acc: 0.0292 - val_loss: 11.4603 - val_acc: 0.0625
Epoch 2/50
137/137 [==============================] - 2s - loss: 11.9409 - acc: 0.0949 - val_loss: 11.7773 - val_acc: 0.0000e+00
Epoch 3/50
137/137 [==============================] - 2s - loss: 12.9744 - acc: 0.0730 - val_loss: 11.2157 - val_acc: 0.1875
Epoch 4/50
137/137 [==============================] - 1s - loss: 13.3462 - acc: 0.1022 - val_loss: 12.4206 - val_acc: 0.0000e+00
Epoch 5/50
137/137 [==============================] - 1s - loss: 13.4079 - acc: 0.0949 - val_loss: 11.0909 - val_acc: 0.3125
Epoch 6/50
137/137 [==============================] - 2s - loss: 12.9061 - acc: 0.1460 - val_loss: 11.1994 - val_acc: 0.2500
Epoch 7/50
137/137 [==============================] - 1s - loss: 12.7879 - acc: 0.1752 - val_loss: 11.0815 - val_acc: 0.3125
Epoch 8/50
137/137 [==============================] - 1s - loss: 12.7571 - acc: 0.1971 - val_loss: 11.0825 - val_acc: 0.3125
Epoch 9/50
137/137 [==============================] - 1s - loss: 12.7325 - acc: 0.2044 - val_loss: 11.0813 - val_acc: 0.3125
Epoch 10/50
137/137 [==============================] - 1s - loss: 12.7182 - acc: 0.2044 - val_loss: 11.0828 - val_acc: 0.3125
Epoch 11/50
137/137 [==============================] - 1s - loss: 12.7115 - acc: 0.2117 - val_loss: 11.0815 - val_acc: 0.3125
Epoch 12/50
137/137 [==============================] - 1s - loss: 12.7076 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 13/50
137/137 [==============================] - 1s - loss: 12.7069 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 14/50
137/137 [==============================] - 1s - loss: 12.7067 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 15/50
137/137 [==============================] - 2s - loss: 12.7066 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 16/50
137/137 [==============================] - 1s - loss: 12.7066 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 17/50
137/137 [==============================] - 1s - loss: 12.7065 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 18/50
137/137 [==============================] - 1s - loss: 12.7065 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 19/50
137/137 [==============================] - 1s - loss: 12.7065 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 20/50
137/137 [==============================] - 1s - loss: 12.7064 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 21/50
137/137 [==============================] - 1s - loss: 12.7064 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 22/50
137/137 [==============================] - 1s - loss: 12.7064 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 23/50
137/137 [==============================] - 1s - loss: 12.7064 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 24/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 25/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 26/50
137/137 [==============================] - 2s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 27/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 28/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 29/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 30/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 31/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 32/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 33/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 34/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 35/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 36/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 37/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 38/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 39/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 40/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 41/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 42/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 43/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 44/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 45/50
137/137 [==============================] - 2s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 46/50
137/137 [==============================] - 2s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 47/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 48/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 49/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Epoch 50/50
137/137 [==============================] - 1s - loss: 12.7063 - acc: 0.2117 - val_loss: 11.0812 - val_acc: 0.3125
Loss : 11.0811958313
Accuracy :0.3125











model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('relu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('relu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_4 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_4 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_6 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 2.7021 - acc: 0.0803 - val_loss: 3.0223 - val_acc: 0.0625
Epoch 2/50
137/137 [==============================] - 0s - loss: 2.9456 - acc: 0.0730 - val_loss: 2.6793 - val_acc: 0.0625
Epoch 3/50
137/137 [==============================] - 1s - loss: 2.4692 - acc: 0.2263 - val_loss: 3.0306 - val_acc: 0.1250
Epoch 4/50
137/137 [==============================] - 1s - loss: 2.6755 - acc: 0.1241 - val_loss: 2.4332 - val_acc: 0.1250
Epoch 5/50
137/137 [==============================] - 1s - loss: 2.1963 - acc: 0.3212 - val_loss: 2.8553 - val_acc: 0.0625
Epoch 6/50
137/137 [==============================] - 0s - loss: 2.5336 - acc: 0.1971 - val_loss: 2.3240 - val_acc: 0.1875
Epoch 7/50
137/137 [==============================] - 1s - loss: 2.0271 - acc: 0.4015 - val_loss: 2.6880 - val_acc: 0.0625
Epoch 8/50
137/137 [==============================] - 0s - loss: 1.9980 - acc: 0.4526 - val_loss: 2.5138 - val_acc: 0.1250
Epoch 9/50
137/137 [==============================] - 0s - loss: 2.1500 - acc: 0.3358 - val_loss: 2.5793 - val_acc: 0.0625
Epoch 10/50
137/137 [==============================] - 0s - loss: 1.8813 - acc: 0.4015 - val_loss: 2.4104 - val_acc: 0.1875
Epoch 11/50
137/137 [==============================] - 0s - loss: 1.8435 - acc: 0.5109 - val_loss: 2.1941 - val_acc: 0.1875
Epoch 12/50
137/137 [==============================] - 0s - loss: 1.6856 - acc: 0.4672 - val_loss: 2.2882 - val_acc: 0.1875
Epoch 13/50
137/137 [==============================] - 0s - loss: 1.6116 - acc: 0.5109 - val_loss: 2.0572 - val_acc: 0.2500
Epoch 14/50
137/137 [==============================] - 1s - loss: 1.3556 - acc: 0.6788 - val_loss: 1.6499 - val_acc: 0.3750
Epoch 15/50
137/137 [==============================] - 0s - loss: 1.0936 - acc: 0.7591 - val_loss: 1.9182 - val_acc: 0.3125
Epoch 16/50
137/137 [==============================] - 0s - loss: 1.3315 - acc: 0.6277 - val_loss: 1.9016 - val_acc: 0.1250
Epoch 17/50
137/137 [==============================] - 0s - loss: 1.3808 - acc: 0.4891 - val_loss: 2.5991 - val_acc: 0.2500
Epoch 18/50
137/137 [==============================] - 0s - loss: 1.8092 - acc: 0.4526 - val_loss: 1.9807 - val_acc: 0.2500
Epoch 19/50
137/137 [==============================] - 0s - loss: 1.0338 - acc: 0.7518 - val_loss: 1.5131 - val_acc: 0.4375
Epoch 20/50
137/137 [==============================] - 0s - loss: 0.8367 - acc: 0.8029 - val_loss: 1.6239 - val_acc: 0.2500
Epoch 21/50
137/137 [==============================] - 0s - loss: 1.1184 - acc: 0.6715 - val_loss: 1.3124 - val_acc: 0.5000
Epoch 22/50
137/137 [==============================] - 0s - loss: 0.8522 - acc: 0.7737 - val_loss: 1.1541 - val_acc: 0.5000
Epoch 23/50
137/137 [==============================] - 0s - loss: 0.7392 - acc: 0.8102 - val_loss: 0.9267 - val_acc: 0.7500
Epoch 24/50
137/137 [==============================] - 0s - loss: 0.6065 - acc: 0.9051 - val_loss: 1.2441 - val_acc: 0.6875
Epoch 25/50
137/137 [==============================] - 0s - loss: 0.5522 - acc: 0.8686 - val_loss: 0.8288 - val_acc: 0.8125
Epoch 26/50
137/137 [==============================] - 0s - loss: 0.4668 - acc: 0.9489 - val_loss: 0.6794 - val_acc: 0.9375
Epoch 27/50
137/137 [==============================] - 0s - loss: 0.4145 - acc: 0.9562 - val_loss: 0.9466 - val_acc: 0.6875
Epoch 28/50
137/137 [==============================] - 0s - loss: 0.3953 - acc: 0.9416 - val_loss: 2.0918 - val_acc: 0.5000
Epoch 29/50
137/137 [==============================] - 0s - loss: 1.6731 - acc: 0.5620 - val_loss: 0.8832 - val_acc: 0.8750
Epoch 30/50
137/137 [==============================] - 0s - loss: 0.4964 - acc: 0.9343 - val_loss: 0.9117 - val_acc: 0.6250
Epoch 31/50
137/137 [==============================] - 0s - loss: 0.4261 - acc: 0.9343 - val_loss: 0.6224 - val_acc: 0.8125
Epoch 32/50
137/137 [==============================] - 0s - loss: 0.3432 - acc: 0.9708 - val_loss: 0.8212 - val_acc: 0.7500
Epoch 33/50
137/137 [==============================] - 0s - loss: 0.6539 - acc: 0.8321 - val_loss: 0.5487 - val_acc: 0.9375
Epoch 34/50
137/137 [==============================] - 1s - loss: 0.2723 - acc: 0.9854 - val_loss: 0.5005 - val_acc: 0.9375
Epoch 35/50
137/137 [==============================] - 1s - loss: 0.2586 - acc: 0.9781 - val_loss: 0.5617 - val_acc: 0.8750
Epoch 36/50
137/137 [==============================] - 1s - loss: 0.2295 - acc: 0.9854 - val_loss: 0.5177 - val_acc: 0.9375
Epoch 37/50
137/137 [==============================] - 0s - loss: 0.2200 - acc: 0.9927 - val_loss: 0.4538 - val_acc: 0.9375
Epoch 38/50
137/137 [==============================] - 0s - loss: 0.2011 - acc: 0.9927 - val_loss: 0.4583 - val_acc: 0.9375
Epoch 39/50
137/137 [==============================] - 0s - loss: 0.2294 - acc: 0.9854 - val_loss: 0.4401 - val_acc: 0.9375
Epoch 40/50
137/137 [==============================] - 0s - loss: 0.1987 - acc: 1.0000 - val_loss: 0.7026 - val_acc: 0.8750
Epoch 41/50
137/137 [==============================] - 1s - loss: 0.2795 - acc: 0.9635 - val_loss: 0.4517 - val_acc: 0.9375
Epoch 42/50
137/137 [==============================] - 2s - loss: 0.1701 - acc: 1.0000 - val_loss: 0.4340 - val_acc: 1.0000
Epoch 43/50
137/137 [==============================] - 1s - loss: 0.1738 - acc: 0.9854 - val_loss: 0.4450 - val_acc: 0.9375
Epoch 44/50
137/137 [==============================] - 1s - loss: 0.1683 - acc: 0.9927 - val_loss: 0.4805 - val_acc: 0.8750
Epoch 45/50
137/137 [==============================] - 0s - loss: 0.1495 - acc: 1.0000 - val_loss: 0.4079 - val_acc: 0.9375
Epoch 46/50
137/137 [==============================] - 1s - loss: 0.1427 - acc: 1.0000 - val_loss: 0.4573 - val_acc: 0.8750
Epoch 47/50
137/137 [==============================] - 1s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.4509 - val_acc: 1.0000
Epoch 48/50
137/137 [==============================] - 1s - loss: 0.1547 - acc: 0.9927 - val_loss: 0.4621 - val_acc: 0.8750
Epoch 49/50
137/137 [==============================] - 1s - loss: 0.1358 - acc: 1.0000 - val_loss: 0.3913 - val_acc: 0.9375
Epoch 50/50
137/137 [==============================] - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.3671 - val_acc: 0.9375
Loss : 0.367139935493
Accuracy :0.9375












model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_7 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_7 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_8 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_9 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 3.6694 - acc: 0.0073 - val_loss: 5.5763 - val_acc: 0.1250
Epoch 2/50
137/137 [==============================] - 0s - loss: 5.4822 - acc: 0.1168 - val_loss: 6.6725 - val_acc: 0.1875
Epoch 3/50
137/137 [==============================] - 0s - loss: 5.1029 - acc: 0.1971 - val_loss: 7.3420 - val_acc: 0.1875
Epoch 4/50
137/137 [==============================] - 0s - loss: 5.6895 - acc: 0.1460 - val_loss: 5.8011 - val_acc: 0.2500
Epoch 5/50
137/137 [==============================] - 0s - loss: 3.9019 - acc: 0.3285 - val_loss: 5.7023 - val_acc: 0.1250
Epoch 6/50
137/137 [==============================] - 0s - loss: 4.1212 - acc: 0.1241 - val_loss: 4.6151 - val_acc: 0.1875
Epoch 7/50
137/137 [==============================] - 0s - loss: 2.8393 - acc: 0.3358 - val_loss: 3.0995 - val_acc: 0.1250
Epoch 8/50
137/137 [==============================] - 0s - loss: 2.0834 - acc: 0.4526 - val_loss: 4.8289 - val_acc: 0.1250
Epoch 9/50
137/137 [==============================] - 0s - loss: 3.3762 - acc: 0.2482 - val_loss: 2.7448 - val_acc: 0.2500
Epoch 10/50
137/137 [==============================] - 0s - loss: 1.8998 - acc: 0.4599 - val_loss: 3.0466 - val_acc: 0.1250
Epoch 11/50
137/137 [==============================] - 0s - loss: 2.2004 - acc: 0.3577 - val_loss: 2.6776 - val_acc: 0.1250
Epoch 12/50
137/137 [==============================] - 0s - loss: 2.0690 - acc: 0.4161 - val_loss: 2.3620 - val_acc: 0.1875
Epoch 13/50
137/137 [==============================] - 0s - loss: 1.8516 - acc: 0.4526 - val_loss: 1.8086 - val_acc: 0.3750
Epoch 14/50
137/137 [==============================] - 1s - loss: 1.3102 - acc: 0.6204 - val_loss: 1.7000 - val_acc: 0.4375
Epoch 15/50
137/137 [==============================] - 1s - loss: 0.9756 - acc: 0.7664 - val_loss: 1.7736 - val_acc: 0.4375
Epoch 16/50
137/137 [==============================] - 0s - loss: 1.3875 - acc: 0.5328 - val_loss: 1.4666 - val_acc: 0.5625
Epoch 17/50
137/137 [==============================] - 1s - loss: 1.0282 - acc: 0.7080 - val_loss: 1.1048 - val_acc: 0.6875
Epoch 18/50
137/137 [==============================] - 0s - loss: 0.8027 - acc: 0.7737 - val_loss: 1.4391 - val_acc: 0.4375
Epoch 19/50
137/137 [==============================] - 0s - loss: 0.8195 - acc: 0.7883 - val_loss: 1.0707 - val_acc: 0.7500
Epoch 20/50
137/137 [==============================] - 0s - loss: 0.9678 - acc: 0.7007 - val_loss: 0.9909 - val_acc: 0.6875
Epoch 21/50
137/137 [==============================] - 0s - loss: 0.5986 - acc: 0.8613 - val_loss: 0.8750 - val_acc: 0.7500
Epoch 22/50
137/137 [==============================] - 0s - loss: 0.4766 - acc: 0.8978 - val_loss: 0.8351 - val_acc: 0.7500
Epoch 23/50
137/137 [==============================] - 1s - loss: 0.3568 - acc: 0.9781 - val_loss: 1.5192 - val_acc: 0.6875
Epoch 24/50
137/137 [==============================] - 1s - loss: 0.6787 - acc: 0.7956 - val_loss: 0.6329 - val_acc: 0.8750
Epoch 25/50
137/137 [==============================] - 1s - loss: 0.3659 - acc: 0.9416 - val_loss: 0.5826 - val_acc: 0.8750
Epoch 26/50
137/137 [==============================] - 1s - loss: 0.2672 - acc: 0.9854 - val_loss: 1.0109 - val_acc: 0.7500
Epoch 27/50
137/137 [==============================] - 1s - loss: 0.6295 - acc: 0.8029 - val_loss: 0.6138 - val_acc: 0.9375
Epoch 28/50
137/137 [==============================] - 1s - loss: 0.2504 - acc: 0.9854 - val_loss: 0.5654 - val_acc: 0.8750
Epoch 29/50
137/137 [==============================] - 1s - loss: 0.2365 - acc: 0.9781 - val_loss: 0.4735 - val_acc: 0.8750
Epoch 30/50
137/137 [==============================] - 1s - loss: 0.2099 - acc: 0.9927 - val_loss: 0.4390 - val_acc: 0.8750
Epoch 31/50
137/137 [==============================] - 1s - loss: 0.1715 - acc: 1.0000 - val_loss: 0.4693 - val_acc: 0.8750
Epoch 32/50
137/137 [==============================] - 1s - loss: 0.1738 - acc: 1.0000 - val_loss: 0.5580 - val_acc: 0.8750
Epoch 33/50
137/137 [==============================] - 1s - loss: 0.1922 - acc: 0.9708 - val_loss: 0.4082 - val_acc: 0.8750
Epoch 34/50
137/137 [==============================] - 1s - loss: 0.1431 - acc: 1.0000 - val_loss: 0.5382 - val_acc: 0.8125
Epoch 35/50
137/137 [==============================] - 1s - loss: 0.1553 - acc: 0.9927 - val_loss: 0.5520 - val_acc: 0.8750
Epoch 36/50
137/137 [==============================] - 1s - loss: 0.1520 - acc: 1.0000 - val_loss: 0.4762 - val_acc: 0.8750
Epoch 37/50
137/137 [==============================] - 1s - loss: 0.1437 - acc: 1.0000 - val_loss: 0.3596 - val_acc: 0.9375
Epoch 38/50
137/137 [==============================] - 1s - loss: 0.1247 - acc: 1.0000 - val_loss: 0.3711 - val_acc: 0.8750
Epoch 39/50
137/137 [==============================] - 1s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.4303 - val_acc: 0.8750
Epoch 40/50
137/137 [==============================] - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.4357 - val_acc: 0.9375
Epoch 41/50
137/137 [==============================] - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.3726 - val_acc: 0.9375
Epoch 42/50
137/137 [==============================] - 1s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.4569 - val_acc: 0.8750
Epoch 43/50
137/137 [==============================] - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.3441 - val_acc: 0.9375
Epoch 44/50
137/137 [==============================] - 0s - loss: 0.0922 - acc: 1.0000 - val_loss: 0.3905 - val_acc: 0.8125
Epoch 45/50
137/137 [==============================] - 0s - loss: 0.1053 - acc: 0.9927 - val_loss: 0.3461 - val_acc: 0.8750
Epoch 46/50
137/137 [==============================] - 1s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.3158 - val_acc: 0.9375
Epoch 47/50
137/137 [==============================] - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.3806 - val_acc: 0.9375
Epoch 48/50
137/137 [==============================] - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.3610 - val_acc: 0.8750
Epoch 49/50
137/137 [==============================] - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.3190 - val_acc: 0.9375
Epoch 50/50
137/137 [==============================] - 1s - loss: 0.0836 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 0.8750
Loss : 0.286853373051
Accuracy :0.875










model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('selu'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('selu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_10 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_10 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_11 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_11 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_12 (Dense)             (None, 14)                7182      
_________________________________________________________________
activation_12 (Activation)   (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 3.9343 - acc: 0.0876 - val_loss: 10.8036 - val_acc: 0.0000e+00
Epoch 2/50
137/137 [==============================] - 1s - loss: 10.4597 - acc: 0.1022 - val_loss: 8.8244 - val_acc: 0.3125
Epoch 3/50
137/137 [==============================] - 1s - loss: 12.4196 - acc: 0.0949 - val_loss: 11.3845 - val_acc: 0.1250
Epoch 4/50
137/137 [==============================] - 1s - loss: 13.9732 - acc: 0.0584 - val_loss: 11.5702 - val_acc: 0.1875
Epoch 5/50
137/137 [==============================] - 2s - loss: 13.8868 - acc: 0.0365 - val_loss: 13.7656 - val_acc: 0.0625
Epoch 6/50
137/137 [==============================] - 1s - loss: 13.6336 - acc: 0.0803 - val_loss: 10.4286 - val_acc: 0.1250
Epoch 7/50
137/137 [==============================] - 1s - loss: 12.9221 - acc: 0.0511 - val_loss: 13.4287 - val_acc: 0.1250
Epoch 8/50
137/137 [==============================] - 1s - loss: 13.6052 - acc: 0.1022 - val_loss: 11.0546 - val_acc: 0.0625
Epoch 9/50
137/137 [==============================] - 1s - loss: 12.0809 - acc: 0.1314 - val_loss: 9.3720 - val_acc: 0.0625
Epoch 10/50
137/137 [==============================] - 2s - loss: 9.0131 - acc: 0.1022 - val_loss: 8.6160 - val_acc: 0.1250
Epoch 11/50
137/137 [==============================] - 1s - loss: 11.6809 - acc: 0.0876 - val_loss: 10.1066 - val_acc: 0.1250
Epoch 12/50
137/137 [==============================] - 1s - loss: 11.0559 - acc: 0.1898 - val_loss: 9.5111 - val_acc: 0.1875
Epoch 13/50
137/137 [==============================] - 1s - loss: 11.7012 - acc: 0.1095 - val_loss: 11.1399 - val_acc: 0.0625
Epoch 14/50
137/137 [==============================] - 1s - loss: 12.6400 - acc: 0.0730 - val_loss: 14.3669 - val_acc: 0.0625
Epoch 15/50
137/137 [==============================] - 1s - loss: 13.3507 - acc: 0.1241 - val_loss: 10.2379 - val_acc: 0.1875
Epoch 16/50
137/137 [==============================] - 1s - loss: 12.6712 - acc: 0.1168 - val_loss: 9.9699 - val_acc: 0.1250
Epoch 17/50
137/137 [==============================] - 1s - loss: 12.2466 - acc: 0.1241 - val_loss: 11.6821 - val_acc: 0.1250
Epoch 18/50
137/137 [==============================] - 1s - loss: 11.8773 - acc: 0.1241 - val_loss: 7.7265 - val_acc: 0.0000e+00
Epoch 19/50
137/137 [==============================] - 1s - loss: 8.9489 - acc: 0.1241 - val_loss: 10.1930 - val_acc: 0.0625
Epoch 20/50
137/137 [==============================] - 1s - loss: 9.7380 - acc: 0.0584 - val_loss: 8.6895 - val_acc: 0.2500
Epoch 21/50
137/137 [==============================] - 1s - loss: 9.7647 - acc: 0.1387 - val_loss: 10.3193 - val_acc: 0.0625
Epoch 22/50
137/137 [==============================] - 1s - loss: 10.8598 - acc: 0.0803 - val_loss: 10.3557 - val_acc: 0.1250
Epoch 23/50
137/137 [==============================] - 1s - loss: 9.5615 - acc: 0.2117 - val_loss: 4.8144 - val_acc: 0.4375
Epoch 24/50
137/137 [==============================] - 1s - loss: 4.8322 - acc: 0.1679 - val_loss: 6.7505 - val_acc: 0.1875
Epoch 25/50
137/137 [==============================] - 1s - loss: 6.8157 - acc: 0.1533 - val_loss: 4.9659 - val_acc: 0.3125
Epoch 26/50
137/137 [==============================] - 0s - loss: 6.3871 - acc: 0.2263 - val_loss: 6.5809 - val_acc: 0.1250
Epoch 27/50
137/137 [==============================] - 1s - loss: 6.3529 - acc: 0.2409 - val_loss: 5.1112 - val_acc: 0.2500
Epoch 28/50
137/137 [==============================] - 1s - loss: 6.1594 - acc: 0.2847 - val_loss: 2.4448 - val_acc: 0.4375
Epoch 29/50
137/137 [==============================] - 1s - loss: 2.8313 - acc: 0.5401 - val_loss: 1.5595 - val_acc: 0.4375
Epoch 30/50
137/137 [==============================] - 1s - loss: 1.6235 - acc: 0.5474 - val_loss: 4.6296 - val_acc: 0.1250
Epoch 31/50
137/137 [==============================] - 0s - loss: 4.6477 - acc: 0.2190 - val_loss: 3.2780 - val_acc: 0.3750
Epoch 32/50
137/137 [==============================] - 1s - loss: 2.5122 - acc: 0.4818 - val_loss: 3.5022 - val_acc: 0.2500
Epoch 33/50
137/137 [==============================] - 1s - loss: 2.3128 - acc: 0.5255 - val_loss: 2.4142 - val_acc: 0.4375
Epoch 34/50
137/137 [==============================] - 0s - loss: 1.3894 - acc: 0.6642 - val_loss: 1.7225 - val_acc: 0.5000
Epoch 35/50
137/137 [==============================] - 1s - loss: 1.5300 - acc: 0.5912 - val_loss: 2.7541 - val_acc: 0.4375
Epoch 36/50
137/137 [==============================] - 1s - loss: 2.5905 - acc: 0.5109 - val_loss: 1.2314 - val_acc: 0.5000
Epoch 37/50
137/137 [==============================] - 1s - loss: 1.1400 - acc: 0.8102 - val_loss: 0.5542 - val_acc: 0.8125
Epoch 38/50
137/137 [==============================] - 0s - loss: 0.7122 - acc: 0.7810 - val_loss: 1.5591 - val_acc: 0.6250
Epoch 39/50
137/137 [==============================] - 0s - loss: 0.9653 - acc: 0.7007 - val_loss: 1.2652 - val_acc: 0.6250
Epoch 40/50
137/137 [==============================] - 1s - loss: 1.0689 - acc: 0.6861 - val_loss: 1.6508 - val_acc: 0.4375
Epoch 41/50
137/137 [==============================] - 1s - loss: 1.2231 - acc: 0.6642 - val_loss: 0.8764 - val_acc: 0.7500
Epoch 42/50
137/137 [==============================] - 1s - loss: 0.3261 - acc: 0.8978 - val_loss: 0.3960 - val_acc: 0.8750
Epoch 43/50
137/137 [==============================] - 1s - loss: 0.1692 - acc: 0.9708 - val_loss: 0.3003 - val_acc: 0.8750
Epoch 44/50
137/137 [==============================] - 1s - loss: 0.1153 - acc: 0.9927 - val_loss: 0.2779 - val_acc: 0.9375
Epoch 45/50
137/137 [==============================] - 1s - loss: 0.0996 - acc: 1.0000 - val_loss: 0.5071 - val_acc: 0.8750
Epoch 46/50
137/137 [==============================] - 1s - loss: 0.0987 - acc: 1.0000 - val_loss: 0.3217 - val_acc: 0.8750
Epoch 47/50
137/137 [==============================] - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.3709 - val_acc: 0.8750
Epoch 48/50
137/137 [==============================] - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 0.3922 - val_acc: 0.9375
Epoch 49/50

137/137 [==============================] - 0s - loss: 0.0992 - acc: 0.9854 - val_loss: 0.3413 - val_acc: 0.8750
Epoch 50/50
137/137 [==============================] - 0s - loss: 0.0820 - acc: 0.9927 - val_loss: 0.5344 - val_acc: 0.8750
Loss : 0.53440785408
Accuracy :0.875









model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('tanh'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('tanh'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_13 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_13 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_14 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 14)                7182      
_________________________________________________________________
activation_15 (Activation)   (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 3.0648 - acc: 0.0657 - val_loss: 5.1554 - val_acc: 0.0000e+00
Epoch 2/50
137/137 [==============================] - 1s - loss: 3.9170 - acc: 0.1022 - val_loss: 3.4104 - val_acc: 0.0625
Epoch 3/50
137/137 [==============================] - 0s - loss: 2.9587 - acc: 0.1460 - val_loss: 2.7898 - val_acc: 0.0625
Epoch 4/50
137/137 [==============================] - 0s - loss: 2.4182 - acc: 0.1752 - val_loss: 2.4920 - val_acc: 0.1875
Epoch 5/50
137/137 [==============================] - 1s - loss: 2.2219 - acc: 0.3139 - val_loss: 2.2950 - val_acc: 0.3750
Epoch 6/50
137/137 [==============================] - 1s - loss: 2.0130 - acc: 0.5839 - val_loss: 2.8548 - val_acc: 0.1250
Epoch 7/50
137/137 [==============================] - 0s - loss: 2.2079 - acc: 0.3504 - val_loss: 2.2027 - val_acc: 0.2500
Epoch 8/50
137/137 [==============================] - 0s - loss: 1.7525 - acc: 0.4307 - val_loss: 1.7049 - val_acc: 0.5625
Epoch 9/50
137/137 [==============================] - 1s - loss: 1.4883 - acc: 0.6058 - val_loss: 1.4548 - val_acc: 0.4375
Epoch 10/50
137/137 [==============================] - 0s - loss: 1.1559 - acc: 0.7299 - val_loss: 1.6405 - val_acc: 0.5000
Epoch 11/50
137/137 [==============================] - 0s - loss: 1.4904 - acc: 0.5328 - val_loss: 1.8123 - val_acc: 0.3750
Epoch 12/50
137/137 [==============================] - 0s - loss: 1.7612 - acc: 0.4891 - val_loss: 2.0608 - val_acc: 0.5000
Epoch 13/50
137/137 [==============================] - 0s - loss: 1.5748 - acc: 0.5766 - val_loss: 1.6149 - val_acc: 0.4375
Epoch 14/50
137/137 [==============================] - 0s - loss: 1.0994 - acc: 0.6861 - val_loss: 2.3857 - val_acc: 0.1875
Epoch 15/50
137/137 [==============================] - 0s - loss: 1.8062 - acc: 0.4672 - val_loss: 1.5511 - val_acc: 0.3125
Epoch 16/50
137/137 [==============================] - 1s - loss: 0.8231 - acc: 0.7883 - val_loss: 1.8609 - val_acc: 0.2500
Epoch 17/50
137/137 [==============================] - 1s - loss: 1.2997 - acc: 0.5912 - val_loss: 0.8755 - val_acc: 0.8125
Epoch 18/50
137/137 [==============================] - 1s - loss: 0.6065 - acc: 0.8978 - val_loss: 1.1277 - val_acc: 0.6250
Epoch 19/50
137/137 [==============================] - 1s - loss: 0.5241 - acc: 0.9197 - val_loss: 0.7803 - val_acc: 0.7500
Epoch 20/50
137/137 [==============================] - 1s - loss: 0.6023 - acc: 0.8759 - val_loss: 0.6604 - val_acc: 0.8750
Epoch 21/50
137/137 [==============================] - 1s - loss: 0.4345 - acc: 0.9343 - val_loss: 0.8070 - val_acc: 0.8125
Epoch 22/50
137/137 [==============================] - 1s - loss: 0.4397 - acc: 0.9197 - val_loss: 0.7367 - val_acc: 0.8125
Epoch 23/50
137/137 [==============================] - 1s - loss: 0.4059 - acc: 0.9343 - val_loss: 0.4985 - val_acc: 0.8750
Epoch 24/50
137/137 [==============================] - 1s - loss: 0.2730 - acc: 0.9927 - val_loss: 0.6669 - val_acc: 0.8750
Epoch 25/50
137/137 [==============================] - 1s - loss: 0.3186 - acc: 0.9635 - val_loss: 1.0710 - val_acc: 0.6250
Epoch 26/50
137/137 [==============================] - 1s - loss: 0.5643 - acc: 0.8978 - val_loss: 0.9108 - val_acc: 0.6250
Epoch 27/50
137/137 [==============================] - 1s - loss: 0.5691 - acc: 0.8540 - val_loss: 0.6265 - val_acc: 0.8750
Epoch 28/50
137/137 [==============================] - 1s - loss: 0.2419 - acc: 0.9927 - val_loss: 0.9379 - val_acc: 0.5000
Epoch 29/50
137/137 [==============================] - 1s - loss: 0.3172 - acc: 0.9489 - val_loss: 0.3945 - val_acc: 0.9375
Epoch 30/50
137/137 [==============================] - 1s - loss: 0.1876 - acc: 1.0000 - val_loss: 0.4312 - val_acc: 0.9375
Epoch 31/50
137/137 [==============================] - 1s - loss: 0.1809 - acc: 0.9927 - val_loss: 0.4474 - val_acc: 0.8750
Epoch 32/50
137/137 [==============================] - 0s - loss: 0.1551 - acc: 1.0000 - val_loss: 0.3697 - val_acc: 0.9375
Epoch 33/50
137/137 [==============================] - 1s - loss: 0.1464 - acc: 1.0000 - val_loss: 0.3952 - val_acc: 0.8750
Epoch 34/50
137/137 [==============================] - 0s - loss: 0.1435 - acc: 1.0000 - val_loss: 0.4501 - val_acc: 0.8750
Epoch 35/50
137/137 [==============================] - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 0.4844 - val_acc: 0.8750
Epoch 36/50
137/137 [==============================] - 0s - loss: 0.1606 - acc: 0.9927 - val_loss: 0.3417 - val_acc: 0.9375
Epoch 37/50
137/137 [==============================] - 1s - loss: 0.1247 - acc: 1.0000 - val_loss: 0.3724 - val_acc: 0.9375
Epoch 38/50
137/137 [==============================] - 0s - loss: 0.1421 - acc: 0.9927 - val_loss: 0.3442 - val_acc: 0.8750
Epoch 39/50
137/137 [==============================] - 1s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.3114 - val_acc: 0.9375
Epoch 40/50
137/137 [==============================] - 0s - loss: 0.1079 - acc: 1.0000 - val_loss: 0.3278 - val_acc: 0.9375
Epoch 41/50
137/137 [==============================] - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 0.3005 - val_acc: 0.9375
Epoch 42/50
137/137 [==============================] - 0s - loss: 0.1024 - acc: 1.0000 - val_loss: 0.3523 - val_acc: 0.9375
Epoch 43/50
137/137 [==============================] - 0s - loss: 0.1220 - acc: 0.9927 - val_loss: 0.2989 - val_acc: 0.9375
Epoch 44/50
137/137 [==============================] - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.3677 - val_acc: 0.9375
Epoch 45/50
137/137 [==============================] - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.3018 - val_acc: 0.9375
Epoch 46/50
137/137 [==============================] - 0s - loss: 0.0941 - acc: 1.0000 - val_loss: 0.3164 - val_acc: 0.9375
Epoch 47/50
137/137 [==============================] - 0s - loss: 0.0921 - acc: 1.0000 - val_loss: 0.3037 - val_acc: 0.9375
Epoch 48/50
137/137 [==============================] - 0s - loss: 0.0868 - acc: 1.0000 - val_loss: 0.2814 - val_acc: 0.9375
Epoch 49/50
137/137 [==============================] - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.3085 - val_acc: 0.8750
Epoch 50/50
137/137 [==============================] - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.2719 - val_acc: 0.9375
Loss : 0.271919190884
Accuracy :0.9375









model = Sequential()
model.add(Dense(512,input_shape=(X_train.shape[1],)))
model.add(Activation('tanh'))
#model.add(Dropout(0.2))
model.add(Dense(512))
model.add(Activation('tanh'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softplus'))

print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)

print("Loss : "+str(loss))

print("Accuracy :"+str(accuracy))

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_16 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_17 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_18 (Dense)             (None, 14)                7182      
_________________________________________________________________
activation_18 (Activation)   (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 2s - loss: 2.6505 - acc: 0.0803 - val_loss: 2.7589 - val_acc: 0.1875
Epoch 2/50
137/137 [==============================] - 1s - loss: 2.7726 - acc: 0.0584 - val_loss: 2.7482 - val_acc: 0.1875
Epoch 3/50
137/137 [==============================] - 0s - loss: 2.6067 - acc: 0.0584 - val_loss: 2.8639 - val_acc: 0.0625
Epoch 4/50
137/137 [==============================] - 1s - loss: 2.6142 - acc: 0.0876 - val_loss: 2.5994 - val_acc: 0.0625
Epoch 5/50
137/137 [==============================] - 1s - loss: 2.5547 - acc: 0.0949 - val_loss: 2.6426 - val_acc: 0.0625
Epoch 6/50
137/137 [==============================] - 1s - loss: 2.5340 - acc: 0.1095 - val_loss: 2.4912 - val_acc: 0.1250
Epoch 7/50
137/137 [==============================] - 1s - loss: 2.3722 - acc: 0.1241 - val_loss: 2.4742 - val_acc: 0.1875
Epoch 8/50
137/137 [==============================] - 1s - loss: 2.4528 - acc: 0.1606 - val_loss: 2.3958 - val_acc: 0.1875
Epoch 9/50
137/137 [==============================] - 1s - loss: 2.5049 - acc: 0.1387 - val_loss: 2.4808 - val_acc: 0.1250
Epoch 10/50
137/137 [==============================] - 1s - loss: 2.4363 - acc: 0.1168 - val_loss: 2.3873 - val_acc: 0.1875
Epoch 11/50
137/137 [==============================] - 1s - loss: 2.2541 - acc: 0.1314 - val_loss: 2.3754 - val_acc: 0.1875
Epoch 12/50
137/137 [==============================] - 1s - loss: 2.3490 - acc: 0.0949 - val_loss: 2.3980 - val_acc: 0.2500
Epoch 13/50
137/137 [==============================] - 1s - loss: 2.3011 - acc: 0.1460 - val_loss: 2.5061 - val_acc: 0.2500
Epoch 14/50
137/137 [==============================] - 1s - loss: 2.3431 - acc: 0.2190 - val_loss: 2.4839 - val_acc: 0.2500
Epoch 15/50
137/137 [==============================] - 1s - loss: 2.2548 - acc: 0.1971 - val_loss: 2.3639 - val_acc: 0.2500
Epoch 16/50
137/137 [==============================] - 1s - loss: 2.1622 - acc: 0.1752 - val_loss: 2.3075 - val_acc: 0.2500
Epoch 17/50
137/137 [==============================] - 1s - loss: 2.1950 - acc: 0.1533 - val_loss: 2.3038 - val_acc: 0.3125
Epoch 18/50
137/137 [==============================] - 1s - loss: 2.1074 - acc: 0.1825 - val_loss: 2.2857 - val_acc: 0.2500
Epoch 19/50
137/137 [==============================] - 1s - loss: 2.1814 - acc: 0.2263 - val_loss: 2.1360 - val_acc: 0.3125
Epoch 20/50
137/137 [==============================] - 1s - loss: 2.0927 - acc: 0.2263 - val_loss: 2.2618 - val_acc: 0.2500
Epoch 21/50
137/137 [==============================] - 0s - loss: 2.1828 - acc: 0.2847 - val_loss: 2.1719 - val_acc: 0.3750
Epoch 22/50
137/137 [==============================] - 1s - loss: 2.0788 - acc: 0.2336 - val_loss: 2.2084 - val_acc: 0.3750
Epoch 23/50
137/137 [==============================] - 1s - loss: 2.0852 - acc: 0.3139 - val_loss: 2.2343 - val_acc: 0.3750
Epoch 24/50
137/137 [==============================] - 0s - loss: 2.1325 - acc: 0.2263 - val_loss: 2.0508 - val_acc: 0.2500
Epoch 25/50
137/137 [==============================] - 1s - loss: 1.9742 - acc: 0.1168 - val_loss: 2.1801 - val_acc: 0.2500
Epoch 26/50
137/137 [==============================] - 1s - loss: 1.9565 - acc: 0.1168 - val_loss: 2.0526 - val_acc: 0.1875
Epoch 27/50
137/137 [==============================] - 1s - loss: 1.8576 - acc: 0.1241 - val_loss: 2.0452 - val_acc: 0.1875
Epoch 28/50
137/137 [==============================] - 1s - loss: 1.8225 - acc: 0.1241 - val_loss: 2.1184 - val_acc: 0.0625
Epoch 29/50
137/137 [==============================] - 1s - loss: 1.8236 - acc: 0.1314 - val_loss: 2.0055 - val_acc: 0.1250
Epoch 30/50
137/137 [==============================] - 1s - loss: 1.8316 - acc: 0.1460 - val_loss: 2.2840 - val_acc: 0.1875
Epoch 31/50
137/137 [==============================] - 1s - loss: 1.8845 - acc: 0.2117 - val_loss: 1.9055 - val_acc: 0.1875
Epoch 32/50
137/137 [==============================] - 1s - loss: 1.6643 - acc: 0.1387 - val_loss: 2.0176 - val_acc: 0.1875
Epoch 33/50
137/137 [==============================] - 1s - loss: 1.7614 - acc: 0.1752 - val_loss: 2.2286 - val_acc: 0.1875
Epoch 34/50
137/137 [==============================] - 0s - loss: 1.8840 - acc: 0.2044 - val_loss: 2.0935 - val_acc: 0.0625
Epoch 35/50
137/137 [==============================] - 1s - loss: 1.6838 - acc: 0.0730 - val_loss: 1.8511 - val_acc: 0.0625
Epoch 36/50
137/137 [==============================] - 0s - loss: 1.6659 - acc: 0.0949 - val_loss: 2.0108 - val_acc: 0.0625
Epoch 37/50
137/137 [==============================] - 0s - loss: 1.7223 - acc: 0.2409 - val_loss: 2.2748 - val_acc: 0.2500
Epoch 38/50
137/137 [==============================] - 0s - loss: 1.7344 - acc: 0.2336 - val_loss: 2.0432 - val_acc: 0.2500
Epoch 39/50
137/137 [==============================] - 0s - loss: 1.7425 - acc: 0.2555 - val_loss: 1.8861 - val_acc: 0.1250
Epoch 40/50
137/137 [==============================] - 0s - loss: 1.6292 - acc: 0.2117 - val_loss: 1.9331 - val_acc: 0.2500
Epoch 41/50
137/137 [==============================] - 0s - loss: 1.5810 - acc: 0.2409 - val_loss: 1.9417 - val_acc: 0.1875
Epoch 42/50
137/137 [==============================] - 1s - loss: 1.6947 - acc: 0.2336 - val_loss: 1.9091 - val_acc: 0.1250
Epoch 43/50
137/137 [==============================] - 0s - loss: 1.7477 - acc: 0.1022 - val_loss: 1.9850 - val_acc: 0.1250
Epoch 44/50
137/137 [==============================] - 0s - loss: 1.6777 - acc: 0.1387 - val_loss: 1.9226 - val_acc: 0.3125
Epoch 45/50
137/137 [==============================] - 1s - loss: 1.7399 - acc: 0.1314 - val_loss: 1.8189 - val_acc: 0.3125
Epoch 46/50
137/137 [==============================] - 1s - loss: 1.6472 - acc: 0.1241 - val_loss: 1.7529 - val_acc: 0.2500
Epoch 47/50
137/137 [==============================] - 1s - loss: 1.5460 - acc: 0.1241 - val_loss: 1.6675 - val_acc: 0.1875
Epoch 48/50
137/137 [==============================] - 0s - loss: 1.4730 - acc: 0.0949 - val_loss: 1.8309 - val_acc: 0.3125
Epoch 49/50
137/137 [==============================] - 0s - loss: 1.5371 - acc: 0.2628 - val_loss: 1.6030 - val_acc: 0.1875
Epoch 50/50
137/137 [==============================] - 0s - loss: 1.3656 - acc: 0.1241 - val_loss: 1.6080 - val_acc: 0.3125
Loss : 1.60801148415
Accuracy :0.3125