Dataset : LFW (First 1000 persons - Non-uniform samples for each person : JPG format) 
-------------------------------------------------------------------------
Images length : 1936
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_13 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_13 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_14 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_15 (Dense)             (None, 1000)              513000    
_________________________________________________________________
activation_15 (Activation)   (None, 1000)              0         
=================================================================
Total params: 12,296,168
Trainable params: 12,296,168
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 1742 samples, validate on 194 samples
Epoch 1/50
1742/1742 [==============================] - 30s - loss: 8.8760 - acc: 0.0195 - val_loss: 6.7985 - val_acc: 0.0412
Epoch 2/50
1742/1742 [==============================] - 18s - loss: 6.5489 - acc: 0.0344 - val_loss: 6.8541 - val_acc: 0.0361
Epoch 3/50
1742/1742 [==============================] - 18s - loss: 6.4099 - acc: 0.0373 - val_loss: 7.0969 - val_acc: 0.0361
Epoch 4/50
1742/1742 [==============================] - 21s - loss: 6.4117 - acc: 0.0356 - val_loss: 7.1760 - val_acc: 0.0361
Epoch 5/50
1742/1742 [==============================] - 18s - loss: 6.3337 - acc: 0.0356 - val_loss: 7.1373 - val_acc: 0.0309
Epoch 6/50
1742/1742 [==============================] - 18s - loss: 6.2617 - acc: 0.0373 - val_loss: 7.3323 - val_acc: 0.0309
Epoch 7/50
1742/1742 [==============================] - 23s - loss: 6.2717 - acc: 0.0419 - val_loss: 7.5072 - val_acc: 0.0361
Epoch 8/50
1742/1742 [==============================] - 18s - loss: 6.1885 - acc: 0.0390 - val_loss: 7.3667 - val_acc: 0.0515
Epoch 9/50
1742/1742 [==============================] - 18s - loss: 6.1504 - acc: 0.0425 - val_loss: 7.5885 - val_acc: 0.0464
Epoch 10/50
1742/1742 [==============================] - 22s - loss: 6.0836 - acc: 0.0465 - val_loss: 8.1764 - val_acc: 0.0361
Epoch 11/50
1742/1742 [==============================] - 19s - loss: 6.0867 - acc: 0.0465 - val_loss: 8.1090 - val_acc: 0.0567
Epoch 12/50
1742/1742 [==============================] - 17s - loss: 6.0764 - acc: 0.0442 - val_loss: 8.0580 - val_acc: 0.0567
Epoch 13/50
1742/1742 [==============================] - 18s - loss: 6.0181 - acc: 0.0459 - val_loss: 7.9692 - val_acc: 0.0412
Epoch 14/50
1742/1742 [==============================] - 21s - loss: 5.9459 - acc: 0.0540 - val_loss: 7.9140 - val_acc: 0.0515
Epoch 15/50
1742/1742 [==============================] - 17s - loss: 5.9615 - acc: 0.0505 - val_loss: 8.8209 - val_acc: 0.0619
Epoch 16/50
1742/1742 [==============================] - 17s - loss: 5.9507 - acc: 0.0505 - val_loss: 8.2495 - val_acc: 0.0567
Epoch 17/50
1742/1742 [==============================] - 17s - loss: 5.9048 - acc: 0.0528 - val_loss: 8.1962 - val_acc: 0.0567
Epoch 18/50
1742/1742 [==============================] - 22s - loss: 5.8648 - acc: 0.0534 - val_loss: 8.4048 - val_acc: 0.0619
Epoch 19/50
1742/1742 [==============================] - 18s - loss: 5.8384 - acc: 0.0505 - val_loss: 8.4065 - val_acc: 0.0567
Epoch 20/50
1742/1742 [==============================] - 17s - loss: 5.8229 - acc: 0.0494 - val_loss: 8.6545 - val_acc: 0.0619
Epoch 21/50
1742/1742 [==============================] - 18s - loss: 5.7915 - acc: 0.0459 - val_loss: 8.4175 - val_acc: 0.0515
Epoch 22/50
1742/1742 [==============================] - 21s - loss: 5.7679 - acc: 0.0522 - val_loss: 8.5579 - val_acc: 0.0567
Epoch 23/50
1742/1742 [==============================] - 17s - loss: 5.7389 - acc: 0.0499 - val_loss: 8.7138 - val_acc: 0.0670
Epoch 24/50
1742/1742 [==============================] - 18s - loss: 5.7338 - acc: 0.0517 - val_loss: 8.9480 - val_acc: 0.0619
Epoch 25/50
1742/1742 [==============================] - 18s - loss: 5.7437 - acc: 0.0511 - val_loss: 8.8294 - val_acc: 0.0619
Epoch 26/50
1742/1742 [==============================] - 22s - loss: 5.7213 - acc: 0.0528 - val_loss: 8.6385 - val_acc: 0.0515
Epoch 27/50
1742/1742 [==============================] - 17s - loss: 5.6938 - acc: 0.0517 - val_loss: 9.0220 - val_acc: 0.0670
Epoch 28/50
1742/1742 [==============================] - 17s - loss: 5.7017 - acc: 0.0551 - val_loss: 8.9111 - val_acc: 0.0619
Epoch 29/50
1742/1742 [==============================] - 17s - loss: 5.7144 - acc: 0.0471 - val_loss: 9.3000 - val_acc: 0.0619
Epoch 30/50
1742/1742 [==============================] - 22s - loss: 5.7294 - acc: 0.0454 - val_loss: 8.7910 - val_acc: 0.0567
Epoch 31/50
1742/1742 [==============================] - 18s - loss: 5.6826 - acc: 0.0545 - val_loss: 8.9615 - val_acc: 0.0619
Epoch 32/50
1742/1742 [==============================] - 17s - loss: 5.6488 - acc: 0.0511 - val_loss: 8.9564 - val_acc: 0.0670
Epoch 33/50
1742/1742 [==============================] - 18s - loss: 5.6447 - acc: 0.0528 - val_loss: 9.0463 - val_acc: 0.0619
Epoch 34/50
1742/1742 [==============================] - 21s - loss: 5.6366 - acc: 0.0471 - val_loss: 9.0523 - val_acc: 0.0670
Epoch 35/50
1742/1742 [==============================] - 18s - loss: 5.6189 - acc: 0.0522 - val_loss: 8.9776 - val_acc: 0.0567
Epoch 36/50
1742/1742 [==============================] - 18s - loss: 5.6385 - acc: 0.0476 - val_loss: 9.2819 - val_acc: 0.0722
Epoch 37/50
1742/1742 [==============================] - 23s - loss: 5.6179 - acc: 0.0551 - val_loss: 9.2159 - val_acc: 0.0619
Epoch 38/50
1742/1742 [==============================] - 18s - loss: 5.5925 - acc: 0.0517 - val_loss: 9.2141 - val_acc: 0.0515
Epoch 39/50
1742/1742 [==============================] - 17s - loss: 5.6415 - acc: 0.0545 - val_loss: 9.1003 - val_acc: 0.0619
Epoch 40/50
1742/1742 [==============================] - 20s - loss: 5.5928 - acc: 0.0563 - val_loss: 9.3673 - val_acc: 0.0619
Epoch 41/50
1742/1742 [==============================] - 20s - loss: 5.5668 - acc: 0.0580 - val_loss: 9.0836 - val_acc: 0.0619
Epoch 42/50
1742/1742 [==============================] - 18s - loss: 5.5631 - acc: 0.0574 - val_loss: 9.2331 - val_acc: 0.0567
Epoch 43/50
1742/1742 [==============================] - 19s - loss: 5.6414 - acc: 0.0476 - val_loss: 9.1015 - val_acc: 0.0515
Epoch 44/50
1742/1742 [==============================] - 21s - loss: 5.5560 - acc: 0.0586 - val_loss: 9.3426 - val_acc: 0.0567
Epoch 45/50
1742/1742 [==============================] - 17s - loss: 5.5757 - acc: 0.0545 - val_loss: 8.9683 - val_acc: 0.0515
Epoch 46/50
1742/1742 [==============================] - 19s - loss: 5.6662 - acc: 0.0505 - val_loss: 9.2915 - val_acc: 0.0619
Epoch 47/50
1742/1742 [==============================] - 21s - loss: 5.5535 - acc: 0.0528 - val_loss: 9.2390 - val_acc: 0.0670
Epoch 48/50
1742/1742 [==============================] - 17s - loss: 5.5477 - acc: 0.0551 - val_loss: 9.4804 - val_acc: 0.0619
Epoch 49/50
1742/1742 [==============================] - 19s - loss: 5.5897 - acc: 0.0505 - val_loss: 9.2785 - val_acc: 0.0567
Epoch 50/50
1742/1742 [==============================] - 22s - loss: 5.5015 - acc: 0.0586 - val_loss: 9.3961 - val_acc: 0.0619
Loss : 9.39606029471
Accuracy :0.0618556701031


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Dataset : LFW (First 40 persons - Non-uniform samples for each person : JPG format) 
-------------------------------------------------------------------------
Images length : 76
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_16 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_17 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_18 (Dense)             (None, 40)                20520     
_________________________________________________________________
activation_18 (Activation)   (None, 40)                0         
=================================================================
Total params: 11,803,688
Trainable params: 11,803,688
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 68 samples, validate on 8 samples
Epoch 1/50
68/68 [==============================] - 6s - loss: 3.8813 - acc: 0.1471 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 2/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 3/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 4/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 5/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 6/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 7/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 8/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 9/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 10/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 11/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 12/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 13/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 14/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 15/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 16/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 17/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 18/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 19/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 20/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 21/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 22/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 23/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 24/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 25/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 26/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 27/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 28/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 29/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 30/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 31/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 32/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 33/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 34/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 35/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 36/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 37/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 38/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 39/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 40/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 41/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 42/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 43/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 44/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 45/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 46/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 47/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 48/50
68/68 [==============================] - 1s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 49/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Epoch 50/50
68/68 [==============================] - 0s - loss: 12.5626 - acc: 0.2206 - val_loss: 12.0886 - val_acc: 0.2500
Loss : 12.0885753632
Accuracy :0.25


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : ATNT (40 persons - Uniform samples for each person : All faces are not recognized : PGM format) 
-------------------------------------------------------------------------

Images length : 263
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_19 (Dense)             (None, 512)               11520512  
_________________________________________________________________
activation_19 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_13 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_20 (Dense)             (None, 512)               262656    
_________________________________________________________________
activation_20 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_14 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 40)                20520     
_________________________________________________________________
activation_21 (Activation)   (None, 40)                0         
=================================================================
Total params: 11,803,688
Trainable params: 11,803,688
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 236 samples, validate on 27 samples
Epoch 1/50
236/236 [==============================] - 6s - loss: 11.3055 - acc: 0.0297 - val_loss: 14.9212 - val_acc: 0.0000e+00
Epoch 2/50
236/236 [==============================] - 2s - loss: 15.0019 - acc: 0.0339 - val_loss: 14.9468 - val_acc: 0.0000e+00
Epoch 3/50
236/236 [==============================] - 2s - loss: 15.1252 - acc: 0.0424 - val_loss: 14.3590 - val_acc: 0.1111
Epoch 4/50
236/236 [==============================] - 2s - loss: 14.9108 - acc: 0.0551 - val_loss: 14.6074 - val_acc: 0.0741
Epoch 5/50
236/236 [==============================] - 2s - loss: 15.3833 - acc: 0.0254 - val_loss: 15.1709 - val_acc: 0.0000e+00
Epoch 6/50
236/236 [==============================] - 2s - loss: 15.3193 - acc: 0.0297 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 7/50
236/236 [==============================] - 2s - loss: 15.2915 - acc: 0.0466 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 8/50
236/236 [==============================] - 2s - loss: 14.9461 - acc: 0.0466 - val_loss: 14.1542 - val_acc: 0.0741
Epoch 9/50
236/236 [==============================] - 2s - loss: 15.0277 - acc: 0.0508 - val_loss: 13.5484 - val_acc: 0.1111
Epoch 10/50
236/236 [==============================] - 2s - loss: 15.1801 - acc: 0.0381 - val_loss: 14.2925 - val_acc: 0.0741
Epoch 11/50
236/236 [==============================] - 3s - loss: 15.5245 - acc: 0.0169 - val_loss: 14.8533 - val_acc: 0.0741
Epoch 12/50
236/236 [==============================] - 2s - loss: 15.4495 - acc: 0.0381 - val_loss: 14.1781 - val_acc: 0.1111
Epoch 13/50
236/236 [==============================] - 3s - loss: 15.2194 - acc: 0.0381 - val_loss: 14.9294 - val_acc: 0.0741
Epoch 14/50
236/236 [==============================] - 3s - loss: 15.6232 - acc: 0.0212 - val_loss: 15.5211 - val_acc: 0.0370
Epoch 15/50
236/236 [==============================] - 3s - loss: 15.1721 - acc: 0.0551 - val_loss: 15.4788 - val_acc: 0.0370
Epoch 16/50
236/236 [==============================] - 2s - loss: 15.3399 - acc: 0.0424 - val_loss: 14.9245 - val_acc: 0.0741
Epoch 17/50
236/236 [==============================] - 2s - loss: 15.3039 - acc: 0.0381 - val_loss: 13.7695 - val_acc: 0.1481
Epoch 18/50
236/236 [==============================] - 2s - loss: 14.9713 - acc: 0.0636 - val_loss: 14.2032 - val_acc: 0.1111
Epoch 19/50
236/236 [==============================] - 2s - loss: 15.0682 - acc: 0.0508 - val_loss: 14.6886 - val_acc: 0.0741
Epoch 20/50
236/236 [==============================] - 2s - loss: 15.2019 - acc: 0.0424 - val_loss: 14.6762 - val_acc: 0.0741
Epoch 21/50
236/236 [==============================] - 2s - loss: 14.9026 - acc: 0.0636 - val_loss: 13.9589 - val_acc: 0.0741
Epoch 22/50
236/236 [==============================] - 2s - loss: 14.9313 - acc: 0.0678 - val_loss: 14.2533 - val_acc: 0.0370
Epoch 23/50
236/236 [==============================] - 2s - loss: 15.0934 - acc: 0.0551 - val_loss: 13.8270 - val_acc: 0.1111
Epoch 24/50
236/236 [==============================] - 2s - loss: 15.0676 - acc: 0.0551 - val_loss: 13.7305 - val_acc: 0.1481
Epoch 25/50
236/236 [==============================] - 2s - loss: 15.0589 - acc: 0.0593 - val_loss: 14.1470 - val_acc: 0.0741
Epoch 26/50
236/236 [==============================] - 2s - loss: 15.0702 - acc: 0.0593 - val_loss: 14.3759 - val_acc: 0.0741
Epoch 27/50
236/236 [==============================] - 2s - loss: 15.0905 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 28/50
236/236 [==============================] - 2s - loss: 15.1270 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 29/50
236/236 [==============================] - 2s - loss: 15.0898 - acc: 0.0593 - val_loss: 14.6744 - val_acc: 0.0741
Epoch 30/50
236/236 [==============================] - 2s - loss: 15.1164 - acc: 0.0593 - val_loss: 14.3274 - val_acc: 0.1111
Epoch 31/50
236/236 [==============================] - 2s - loss: 15.1280 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 32/50
236/236 [==============================] - 2s - loss: 15.0809 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 33/50
236/236 [==============================] - 2s - loss: 15.0539 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 34/50
236/236 [==============================] - 2s - loss: 15.0290 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 35/50
236/236 [==============================] - 3s - loss: 15.0301 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 36/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 37/50
236/236 [==============================] - 2s - loss: 15.0519 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 38/50
236/236 [==============================] - 3s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 39/50
236/236 [==============================] - 4s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 40/50
236/236 [==============================] - 2s - loss: 15.0254 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 41/50
236/236 [==============================] - 2s - loss: 15.0772 - acc: 0.0593 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 42/50
236/236 [==============================] - 2s - loss: 15.0283 - acc: 0.0636 - val_loss: 14.6645 - val_acc: 0.0741
Epoch 43/50
236/236 [==============================] - 2s - loss: 15.1621 - acc: 0.0593 - val_loss: 14.9242 - val_acc: 0.0741
Epoch 44/50
236/236 [==============================] - 2s - loss: 15.1230 - acc: 0.0551 - val_loss: 14.3273 - val_acc: 0.1111
Epoch 45/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 46/50
236/236 [==============================] - 2s - loss: 15.0629 - acc: 0.0551 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 47/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 48/50
236/236 [==============================] - 2s - loss: 15.0254 - acc: 0.0678 - val_loss: 14.4264 - val_acc: 0.0741
Epoch 49/50
236/236 [==============================] - 2s - loss: 15.0578 - acc: 0.0636 - val_loss: 14.3272 - val_acc: 0.1111
Epoch 50/50
236/236 [==============================] - 2s - loss: 15.0253 - acc: 0.0678 - val_loss: 14.3272 - val_acc: 0.1111
Loss : 14.3272008896
Accuracy :0.111111111939



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : LFW SMALL - GOOD  (50 persons - All having number of images > 10 - In JPEG format) 
-------------------------------------------------------------------------

model = Sequential()
model.add(Dense(128,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))


model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/10
526/526 [==============================] - 3s - loss: 4.3456 - acc: 0.0209 - val_loss: 5.6132 - val_acc: 0.0169
Epoch 2/10
526/526 [==============================] - 0s - loss: 4.2532 - acc: 0.0456 - val_loss: 4.8437 - val_acc: 0.0000e+00
Epoch 3/10
526/526 [==============================] - 0s - loss: 4.0466 - acc: 0.0361 - val_loss: 4.8265 - val_acc: 0.0339
Epoch 4/10
526/526 [==============================] - 0s - loss: 4.0122 - acc: 0.0456 - val_loss: 4.1759 - val_acc: 0.0508
Epoch 5/10
526/526 [==============================] - 0s - loss: 3.8102 - acc: 0.0684 - val_loss: 4.2375 - val_acc: 0.0169
Epoch 6/10
526/526 [==============================] - 0s - loss: 3.8019 - acc: 0.0627 - val_loss: 4.6711 - val_acc: 0.0000e+00
Epoch 7/10
526/526 [==============================] - 0s - loss: 3.7289 - acc: 0.0989 - val_loss: 4.5663 - val_acc: 0.0169
Epoch 8/10
526/526 [==============================] - 0s - loss: 3.7125 - acc: 0.0951 - val_loss: 4.0555 - val_acc: 0.0339
Epoch 9/10
526/526 [==============================] - 0s - loss: 3.5870 - acc: 0.1141 - val_loss: 4.1912 - val_acc: 0.0169
Epoch 10/10
526/526 [==============================] - 1s - loss: 3.4993 - acc: 0.1331 - val_loss: 4.0920 - val_acc: 0.0847
59/59 [==============================] - 0s     

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("Accuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/10
526/526 [==============================] - 4s - loss: 3.5000 - acc: 0.1084 - val_loss: 4.2573 - val_acc: 0.0508
Epoch 2/10
526/526 [==============================] - 1s - loss: 3.3488 - acc: 0.1578 - val_loss: 5.2315 - val_acc: 0.0000e+00
Epoch 3/10
526/526 [==============================] - 1s - loss: 3.6046 - acc: 0.1426 - val_loss: 4.1777 - val_acc: 0.0339
Epoch 4/10
526/526 [==============================] - 0s - loss: 3.3260 - acc: 0.1692 - val_loss: 3.7488 - val_acc: 0.0339
Epoch 5/10
526/526 [==============================] - 0s - loss: 3.1370 - acc: 0.1901 - val_loss: 5.5753 - val_acc: 0.0508
Epoch 6/10
526/526 [==============================] - 0s - loss: 3.7491 - acc: 0.1502 - val_loss: 4.3526 - val_acc: 0.0169
Epoch 7/10
526/526 [==============================] - 0s - loss: 3.1376 - acc: 0.2319 - val_loss: 4.1007 - val_acc: 0.0847
Epoch 8/10
526/526 [==============================] - 0s - loss: 3.0683 - acc: 0.2414 - val_loss: 4.9063 - val_acc: 0.0339
Epoch 9/10
526/526 [==============================] - 0s - loss: 3.1362 - acc: 0.2376 - val_loss: 4.6447 - val_acc: 0.0000e+00
Epoch 10/10
526/526 [==============================] - 0s - loss: 3.1748 - acc: 0.2129 - val_loss: 5.5766 - val_acc: 0.0339
32/59 [===============>..............] - ETA: 0sAccuracy : 0.0338983050847

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=50, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/50
526/526 [==============================] - 2s - loss: 3.4424 - acc: 0.1768 - val_loss: 3.7278 - val_acc: 0.0678
Epoch 2/50
526/526 [==============================] - 1s - loss: 2.8205 - acc: 0.2890 - val_loss: 5.3244 - val_acc: 0.0508
Epoch 3/50
526/526 [==============================] - 1s - loss: 3.2729 - acc: 0.2186 - val_loss: 4.7578 - val_acc: 0.0000e+00
Epoch 4/50
526/526 [==============================] - 0s - loss: 3.3322 - acc: 0.1844 - val_loss: 4.6859 - val_acc: 0.0000e+00
Epoch 5/50
526/526 [==============================] - 1s - loss: 3.1315 - acc: 0.2548 - val_loss: 4.2128 - val_acc: 0.0508
Epoch 6/50
526/526 [==============================] - 1s - loss: 3.0174 - acc: 0.2700 - val_loss: 4.4012 - val_acc: 0.0169
Epoch 7/50
526/526 [==============================] - 0s - loss: 2.6937 - acc: 0.3194 - val_loss: 3.8243 - val_acc: 0.1356
Epoch 8/50
526/526 [==============================] - 0s - loss: 2.4239 - acc: 0.4087 - val_loss: 3.9408 - val_acc: 0.0678
Epoch 9/50
526/526 [==============================] - 0s - loss: 2.4757 - acc: 0.3536 - val_loss: 4.2514 - val_acc: 0.0847
Epoch 10/50
526/526 [==============================] - 1s - loss: 2.5531 - acc: 0.3327 - val_loss: 6.1066 - val_acc: 0.0169
Epoch 11/50
526/526 [==============================] - 0s - loss: 3.4689 - acc: 0.2700 - val_loss: 3.7984 - val_acc: 0.0678
Epoch 12/50
526/526 [==============================] - 0s - loss: 2.6341 - acc: 0.3745 - val_loss: 5.0049 - val_acc: 0.0847
Epoch 13/50
526/526 [==============================] - 0s - loss: 2.7328 - acc: 0.3175 - val_loss: 3.9860 - val_acc: 0.0339
Epoch 14/50
526/526 [==============================] - 0s - loss: 2.3969 - acc: 0.3821 - val_loss: 4.2373 - val_acc: 0.0169
Epoch 15/50
526/526 [==============================] - 0s - loss: 2.3789 - acc: 0.3669 - val_loss: 4.4909 - val_acc: 0.0508
Epoch 16/50
526/526 [==============================] - 0s - loss: 2.4951 - acc: 0.3669 - val_loss: 3.7294 - val_acc: 0.1186
Epoch 17/50
526/526 [==============================] - 0s - loss: 2.2700 - acc: 0.4316 - val_loss: 4.1497 - val_acc: 0.0847
Epoch 18/50
526/526 [==============================] - 0s - loss: 2.2731 - acc: 0.4163 - val_loss: 3.7061 - val_acc: 0.1017
Epoch 19/50
526/526 [==============================] - 0s - loss: 2.1508 - acc: 0.4544 - val_loss: 4.1401 - val_acc: 0.1017
Epoch 20/50
526/526 [==============================] - 0s - loss: 2.2438 - acc: 0.4259 - val_loss: 3.9011 - val_acc: 0.1186
Epoch 21/50
526/526 [==============================] - 0s - loss: 2.0195 - acc: 0.4962 - val_loss: 4.3006 - val_acc: 0.1017
Epoch 22/50
526/526 [==============================] - 0s - loss: 2.1686 - acc: 0.4468 - val_loss: 4.4132 - val_acc: 0.1356
Epoch 23/50
526/526 [==============================] - 0s - loss: 2.1226 - acc: 0.4715 - val_loss: 4.3951 - val_acc: 0.1017
Epoch 24/50
526/526 [==============================] - 0s - loss: 2.0927 - acc: 0.4354 - val_loss: 4.1587 - val_acc: 0.0847
Epoch 25/50
526/526 [==============================] - 0s - loss: 2.0065 - acc: 0.4620 - val_loss: 4.1087 - val_acc: 0.0847
Epoch 26/50
526/526 [==============================] - 0s - loss: 1.9119 - acc: 0.5152 - val_loss: 3.9031 - val_acc: 0.1864
Epoch 27/50
526/526 [==============================] - 0s - loss: 2.0004 - acc: 0.5000 - val_loss: 3.7811 - val_acc: 0.1356
Epoch 28/50
526/526 [==============================] - 0s - loss: 1.8696 - acc: 0.5437 - val_loss: 4.1632 - val_acc: 0.1356
Epoch 29/50
526/526 [==============================] - 0s - loss: 1.9505 - acc: 0.4943 - val_loss: 4.4349 - val_acc: 0.0678
Epoch 30/50
526/526 [==============================] - 0s - loss: 1.9175 - acc: 0.5076 - val_loss: 4.0454 - val_acc: 0.0678
Epoch 31/50
526/526 [==============================] - 0s - loss: 1.8703 - acc: 0.5418 - val_loss: 5.8376 - val_acc: 0.0678
Epoch 32/50
526/526 [==============================] - 0s - loss: 2.1659 - acc: 0.5095 - val_loss: 4.3538 - val_acc: 0.1186
Epoch 33/50
526/526 [==============================] - 0s - loss: 2.0050 - acc: 0.5133 - val_loss: 4.3811 - val_acc: 0.0508
Epoch 34/50
526/526 [==============================] - 0s - loss: 1.8199 - acc: 0.5361 - val_loss: 4.3517 - val_acc: 0.0678
Epoch 35/50
526/526 [==============================] - 0s - loss: 1.9613 - acc: 0.5019 - val_loss: 4.0983 - val_acc: 0.0678
Epoch 36/50
526/526 [==============================] - 0s - loss: 1.7180 - acc: 0.5760 - val_loss: 4.3693 - val_acc: 0.0847
Epoch 37/50
526/526 [==============================] - 0s - loss: 1.7236 - acc: 0.5532 - val_loss: 3.9760 - val_acc: 0.1186
Epoch 38/50
526/526 [==============================] - 0s - loss: 1.6252 - acc: 0.5798 - val_loss: 4.0919 - val_acc: 0.1186
Epoch 39/50
526/526 [==============================] - 0s - loss: 1.5616 - acc: 0.5989 - val_loss: 4.6419 - val_acc: 0.0678
Epoch 40/50
526/526 [==============================] - 0s - loss: 1.7251 - acc: 0.5532 - val_loss: 3.9010 - val_acc: 0.0847
Epoch 41/50
526/526 [==============================] - 0s - loss: 1.6790 - acc: 0.5627 - val_loss: 3.5269 - val_acc: 0.0847
Epoch 42/50
526/526 [==============================] - 0s - loss: 1.4476 - acc: 0.6293 - val_loss: 4.0257 - val_acc: 0.1525
Epoch 43/50
526/526 [==============================] - 0s - loss: 1.4963 - acc: 0.6217 - val_loss: 5.3686 - val_acc: 0.0847
Epoch 44/50
526/526 [==============================] - 0s - loss: 1.8616 - acc: 0.5817 - val_loss: 3.5864 - val_acc: 0.1525
Epoch 45/50
526/526 [==============================] - 0s - loss: 1.3996 - acc: 0.6350 - val_loss: 4.0845 - val_acc: 0.1525
Epoch 46/50
526/526 [==============================] - 0s - loss: 1.4612 - acc: 0.6388 - val_loss: 3.6215 - val_acc: 0.1525
Epoch 47/50
526/526 [==============================] - 1s - loss: 1.2218 - acc: 0.6996 - val_loss: 3.7574 - val_acc: 0.1186
Epoch 48/50
526/526 [==============================] - 0s - loss: 1.2754 - acc: 0.6806 - val_loss: 3.9975 - val_acc: 0.1186
Epoch 49/50
526/526 [==============================] - 0s - loss: 1.3228 - acc: 0.6692 - val_loss: 4.2648 - val_acc: 0.1356
Epoch 50/50
526/526 [==============================] - 0s - loss: 1.2278 - acc: 0.7129 - val_loss: 3.8127 - val_acc: 0.1525
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.15254237326

model.compile(loss='sparse_categorical_crossentropy', optimizer="sgd", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=64, nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/100
526/526 [==============================] - 4s - loss: 1.2642 - acc: 0.6787 - val_loss: 5.9221 - val_acc: 0.0678
Epoch 2/100
526/526 [==============================] - 0s - loss: 1.6634 - acc: 0.6388 - val_loss: 3.9648 - val_acc: 0.1356
Epoch 3/100
526/526 [==============================] - 0s - loss: 1.3247 - acc: 0.6749 - val_loss: 3.7628 - val_acc: 0.1186
Epoch 4/100
526/526 [==============================] - 0s - loss: 1.1211 - acc: 0.7243 - val_loss: 3.7223 - val_acc: 0.1525
Epoch 5/100
526/526 [==============================] - 0s - loss: 1.0503 - acc: 0.7586 - val_loss: 4.5698 - val_acc: 0.1017
Epoch 6/100
526/526 [==============================] - 0s - loss: 1.2892 - acc: 0.6806 - val_loss: 4.6337 - val_acc: 0.1356
Epoch 7/100
526/526 [==============================] - 0s - loss: 1.3053 - acc: 0.6844 - val_loss: 3.7494 - val_acc: 0.1356
Epoch 8/100
526/526 [==============================] - 0s - loss: 1.0312 - acc: 0.7548 - val_loss: 3.9890 - val_acc: 0.1356
Epoch 9/100
526/526 [==============================] - 0s - loss: 1.0564 - acc: 0.7738 - val_loss: 3.8090 - val_acc: 0.1695
Epoch 10/100
526/526 [==============================] - 0s - loss: 1.2007 - acc: 0.6787 - val_loss: 3.9195 - val_acc: 0.1695
Epoch 11/100
526/526 [==============================] - 0s - loss: 1.0874 - acc: 0.7300 - val_loss: 3.6650 - val_acc: 0.2203
Epoch 12/100
526/526 [==============================] - 0s - loss: 0.9096 - acc: 0.7928 - val_loss: 3.6133 - val_acc: 0.2034
Epoch 13/100
526/526 [==============================] - 0s - loss: 0.9400 - acc: 0.7662 - val_loss: 4.1190 - val_acc: 0.1186
Epoch 14/100
526/526 [==============================] - 0s - loss: 1.0259 - acc: 0.7490 - val_loss: 4.1199 - val_acc: 0.1356
Epoch 15/100
526/526 [==============================] - 0s - loss: 1.2773 - acc: 0.7110 - val_loss: 3.8035 - val_acc: 0.1186
Epoch 16/100
526/526 [==============================] - 0s - loss: 0.9145 - acc: 0.8004 - val_loss: 3.9489 - val_acc: 0.1186
Epoch 17/100
526/526 [==============================] - 0s - loss: 0.8410 - acc: 0.8118 - val_loss: 3.5650 - val_acc: 0.2203
Epoch 18/100
526/526 [==============================] - 0s - loss: 0.8938 - acc: 0.7947 - val_loss: 4.1428 - val_acc: 0.1186
Epoch 19/100
526/526 [==============================] - 0s - loss: 1.0292 - acc: 0.7357 - val_loss: 3.7427 - val_acc: 0.1356
Epoch 20/100
526/526 [==============================] - 0s - loss: 0.9896 - acc: 0.7605 - val_loss: 3.8049 - val_acc: 0.1356
Epoch 21/100
526/526 [==============================] - 0s - loss: 0.9451 - acc: 0.7719 - val_loss: 3.3177 - val_acc: 0.2373
Epoch 22/100
526/526 [==============================] - 0s - loss: 0.7789 - acc: 0.8536 - val_loss: 3.3956 - val_acc: 0.2034
Epoch 23/100
526/526 [==============================] - 0s - loss: 0.7694 - acc: 0.8289 - val_loss: 4.4055 - val_acc: 0.1186
Epoch 24/100
526/526 [==============================] - 1s - loss: 0.8495 - acc: 0.8137 - val_loss: 4.3330 - val_acc: 0.1186
Epoch 25/100
526/526 [==============================] - 0s - loss: 0.8671 - acc: 0.8099 - val_loss: 3.3240 - val_acc: 0.2881
Epoch 26/100
526/526 [==============================] - 0s - loss: 0.7455 - acc: 0.8460 - val_loss: 4.3340 - val_acc: 0.1695
Epoch 27/100
526/526 [==============================] - 0s - loss: 1.1851 - acc: 0.7357 - val_loss: 3.6501 - val_acc: 0.1356
Epoch 28/100
526/526 [==============================] - 0s - loss: 0.7901 - acc: 0.8213 - val_loss: 4.6090 - val_acc: 0.1356
Epoch 29/100
526/526 [==============================] - 0s - loss: 0.8194 - acc: 0.8270 - val_loss: 3.6600 - val_acc: 0.2203
Epoch 30/100
526/526 [==============================] - 0s - loss: 0.6740 - acc: 0.8783 - val_loss: 3.8488 - val_acc: 0.2203
Epoch 31/100
526/526 [==============================] - 0s - loss: 0.6892 - acc: 0.8764 - val_loss: 3.7510 - val_acc: 0.2034
Epoch 32/100
526/526 [==============================] - 0s - loss: 0.7083 - acc: 0.8783 - val_loss: 3.2177 - val_acc: 0.2203
Epoch 33/100
526/526 [==============================] - 0s - loss: 0.6182 - acc: 0.8821 - val_loss: 3.5354 - val_acc: 0.1864
Epoch 34/100
526/526 [==============================] - 0s - loss: 0.6110 - acc: 0.8840 - val_loss: 3.8481 - val_acc: 0.2373
Epoch 35/100
526/526 [==============================] - 0s - loss: 0.6263 - acc: 0.8783 - val_loss: 5.1863 - val_acc: 0.0678
Epoch 36/100
526/526 [==============================] - 0s - loss: 0.9329 - acc: 0.7700 - val_loss: 4.7938 - val_acc: 0.1356
Epoch 37/100
526/526 [==============================] - 0s - loss: 0.8695 - acc: 0.8042 - val_loss: 3.5721 - val_acc: 0.2373
Epoch 38/100
526/526 [==============================] - 0s - loss: 0.6112 - acc: 0.8878 - val_loss: 4.0780 - val_acc: 0.1186
Epoch 39/100
526/526 [==============================] - 0s - loss: 0.6509 - acc: 0.8764 - val_loss: 3.5682 - val_acc: 0.2542
Epoch 40/100
526/526 [==============================] - 0s - loss: 0.5217 - acc: 0.9144 - val_loss: 3.6885 - val_acc: 0.2034
Epoch 41/100
526/526 [==============================] - 0s - loss: 0.5392 - acc: 0.8954 - val_loss: 3.4325 - val_acc: 0.2373
Epoch 42/100
526/526 [==============================] - 0s - loss: 0.5110 - acc: 0.9125 - val_loss: 4.1316 - val_acc: 0.1864
Epoch 43/100
526/526 [==============================] - 0s - loss: 0.6322 - acc: 0.8840 - val_loss: 3.7461 - val_acc: 0.2034
Epoch 44/100
526/526 [==============================] - 0s - loss: 0.4976 - acc: 0.9297 - val_loss: 3.7588 - val_acc: 0.2203
Epoch 45/100
526/526 [==============================] - 0s - loss: 0.4766 - acc: 0.9278 - val_loss: 4.0636 - val_acc: 0.1695
Epoch 46/100
526/526 [==============================] - 0s - loss: 0.7503 - acc: 0.8631 - val_loss: 3.6113 - val_acc: 0.2373
Epoch 47/100
526/526 [==============================] - 0s - loss: 0.4998 - acc: 0.9354 - val_loss: 3.7159 - val_acc: 0.1695
Epoch 48/100
526/526 [==============================] - 0s - loss: 0.4977 - acc: 0.9221 - val_loss: 3.7000 - val_acc: 0.2034
Epoch 49/100
526/526 [==============================] - 0s - loss: 0.4717 - acc: 0.9316 - val_loss: 4.7497 - val_acc: 0.1525
Epoch 50/100
526/526 [==============================] - 1s - loss: 0.6078 - acc: 0.8726 - val_loss: 3.2380 - val_acc: 0.2712
Epoch 51/100
526/526 [==============================] - 0s - loss: 0.4017 - acc: 0.9468 - val_loss: 3.1883 - val_acc: 0.2712
Epoch 52/100
526/526 [==============================] - 0s - loss: 0.4068 - acc: 0.9544 - val_loss: 3.2930 - val_acc: 0.2712
Epoch 53/100
526/526 [==============================] - 0s - loss: 0.3921 - acc: 0.9544 - val_loss: 3.3287 - val_acc: 0.2712
Epoch 54/100
526/526 [==============================] - 0s - loss: 0.3896 - acc: 0.9563 - val_loss: 4.6032 - val_acc: 0.2203
Epoch 55/100
526/526 [==============================] - 0s - loss: 1.0231 - acc: 0.8156 - val_loss: 3.5519 - val_acc: 0.2203
Epoch 56/100
526/526 [==============================] - 0s - loss: 0.4653 - acc: 0.9487 - val_loss: 3.5094 - val_acc: 0.2881
Epoch 57/100
526/526 [==============================] - 0s - loss: 0.3898 - acc: 0.9582 - val_loss: 3.9603 - val_acc: 0.1356
Epoch 58/100
526/526 [==============================] - 0s - loss: 0.6231 - acc: 0.8650 - val_loss: 3.6630 - val_acc: 0.2373
Epoch 59/100
526/526 [==============================] - 0s - loss: 0.3981 - acc: 0.9563 - val_loss: 3.1190 - val_acc: 0.3559
Epoch 60/100
526/526 [==============================] - 0s - loss: 0.3432 - acc: 0.9715 - val_loss: 3.8199 - val_acc: 0.1186
Epoch 61/100
526/526 [==============================] - 0s - loss: 0.4114 - acc: 0.9335 - val_loss: 4.1469 - val_acc: 0.1356
Epoch 62/100
526/526 [==============================] - 0s - loss: 0.4179 - acc: 0.9297 - val_loss: 3.4904 - val_acc: 0.2712
Epoch 63/100
526/526 [==============================] - 0s - loss: 0.3499 - acc: 0.9582 - val_loss: 3.3167 - val_acc: 0.2712
Epoch 64/100
526/526 [==============================] - 0s - loss: 0.3356 - acc: 0.9715 - val_loss: 3.2701 - val_acc: 0.2373
Epoch 65/100
526/526 [==============================] - 0s - loss: 0.3493 - acc: 0.9620 - val_loss: 3.2205 - val_acc: 0.3051
Epoch 66/100
526/526 [==============================] - 1s - loss: 0.3368 - acc: 0.9658 - val_loss: 3.2189 - val_acc: 0.2542
Epoch 67/100
526/526 [==============================] - 0s - loss: 0.3201 - acc: 0.9696 - val_loss: 3.2326 - val_acc: 0.3220
Epoch 68/100
526/526 [==============================] - 1s - loss: 0.2965 - acc: 0.9829 - val_loss: 3.4450 - val_acc: 0.2203
Epoch 69/100
526/526 [==============================] - 0s - loss: 0.3275 - acc: 0.9544 - val_loss: 4.0909 - val_acc: 0.1186
Epoch 70/100
526/526 [==============================] - 0s - loss: 0.4315 - acc: 0.9259 - val_loss: 3.5124 - val_acc: 0.2881
Epoch 71/100
526/526 [==============================] - 0s - loss: 0.3270 - acc: 0.9525 - val_loss: 3.8139 - val_acc: 0.1695
Epoch 72/100
526/526 [==============================] - 1s - loss: 0.3707 - acc: 0.9506 - val_loss: 3.7695 - val_acc: 0.1864
Epoch 73/100
526/526 [==============================] - 0s - loss: 0.3741 - acc: 0.9430 - val_loss: 3.2835 - val_acc: 0.2712
Epoch 74/100
526/526 [==============================] - 0s - loss: 0.2634 - acc: 0.9886 - val_loss: 3.1591 - val_acc: 0.3051
Epoch 75/100
526/526 [==============================] - 0s - loss: 0.2759 - acc: 0.9772 - val_loss: 3.2590 - val_acc: 0.2373
Epoch 76/100
526/526 [==============================] - 0s - loss: 0.2763 - acc: 0.9810 - val_loss: 3.1903 - val_acc: 0.3051
Epoch 77/100
526/526 [==============================] - 0s - loss: 0.2510 - acc: 0.9905 - val_loss: 3.3822 - val_acc: 0.2542
Epoch 78/100
526/526 [==============================] - 1s - loss: 0.3054 - acc: 0.9772 - val_loss: 3.4774 - val_acc: 0.1864
Epoch 79/100
526/526 [==============================] - 1s - loss: 0.3590 - acc: 0.9506 - val_loss: 3.4670 - val_acc: 0.2881
Epoch 80/100
526/526 [==============================] - 1s - loss: 0.2591 - acc: 0.9791 - val_loss: 3.5594 - val_acc: 0.2542
Epoch 81/100
526/526 [==============================] - 1s - loss: 0.3735 - acc: 0.9392 - val_loss: 3.3879 - val_acc: 0.2542
Epoch 82/100
526/526 [==============================] - 1s - loss: 0.2619 - acc: 0.9791 - val_loss: 3.4444 - val_acc: 0.2712
Epoch 83/100
526/526 [==============================] - 1s - loss: 0.2675 - acc: 0.9734 - val_loss: 3.8936 - val_acc: 0.2373
Epoch 84/100
526/526 [==============================] - 0s - loss: 0.3561 - acc: 0.9563 - val_loss: 3.3732 - val_acc: 0.3051
Epoch 85/100
526/526 [==============================] - 0s - loss: 0.2247 - acc: 0.9886 - val_loss: 3.2430 - val_acc: 0.2373
Epoch 86/100
526/526 [==============================] - 0s - loss: 0.2286 - acc: 0.9924 - val_loss: 3.1362 - val_acc: 0.3559
Epoch 87/100
526/526 [==============================] - 0s - loss: 0.2229 - acc: 0.9943 - val_loss: 3.3249 - val_acc: 0.3051
Epoch 88/100
526/526 [==============================] - 0s - loss: 0.2209 - acc: 0.9867 - val_loss: 3.6700 - val_acc: 0.2542
Epoch 89/100
526/526 [==============================] - 0s - loss: 0.2775 - acc: 0.9696 - val_loss: 3.6555 - val_acc: 0.2203
Epoch 90/100
526/526 [==============================] - 1s - loss: 0.2325 - acc: 0.9867 - val_loss: 3.4558 - val_acc: 0.2542
Epoch 91/100
526/526 [==============================] - 0s - loss: 0.2255 - acc: 0.9810 - val_loss: 3.3554 - val_acc: 0.2881
Epoch 92/100
526/526 [==============================] - 0s - loss: 0.2076 - acc: 0.9905 - val_loss: 3.2934 - val_acc: 0.3051
Epoch 93/100
526/526 [==============================] - 0s - loss: 0.1981 - acc: 0.9962 - val_loss: 3.3076 - val_acc: 0.2881
Epoch 94/100
526/526 [==============================] - 0s - loss: 0.1902 - acc: 0.9981 - val_loss: 3.7088 - val_acc: 0.1695
Epoch 95/100
526/526 [==============================] - 0s - loss: 0.2231 - acc: 0.9829 - val_loss: 3.3317 - val_acc: 0.2712
Epoch 96/100
526/526 [==============================] - 0s - loss: 0.2023 - acc: 0.9905 - val_loss: 3.2898 - val_acc: 0.3051
Epoch 97/100
526/526 [==============================] - 0s - loss: 0.1956 - acc: 0.9943 - val_loss: 3.4920 - val_acc: 0.2203
Epoch 98/100
526/526 [==============================] - 0s - loss: 0.1956 - acc: 0.9962 - val_loss: 3.5599 - val_acc: 0.2881
Epoch 99/100
526/526 [==============================] - 0s - loss: 0.2111 - acc: 0.9829 - val_loss: 3.4665 - val_acc: 0.2542
Epoch 100/100
526/526 [==============================] - 0s - loss: 0.1960 - acc: 0.9943 - val_loss: 3.2468 - val_acc: 0.2881
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.288135594231



model = Sequential()
model.add(Dense(128,input_shape=(X_train.shape[1],)))
model.add(Activation('elu'))
#model.add(Dropout(0.2))
model.add(Dense(nb_classes))
model.add(Activation('elu'))



model.compile(loss='sparse_categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=128, nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))
loss, accuracy = model.evaluate(X_test,Y_test, verbose=1)
print("\nAccuracy : "+str(accuracy))

C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 526 samples, validate on 59 samples
Epoch 1/100
526/526 [==============================] - 18s - loss: 11.2362 - acc: 0.0152 - val_loss: 12.4327 - val_acc: 0.0339
Epoch 2/100
526/526 [==============================] - 0s - loss: 11.0312 - acc: 0.0171 - val_loss: 12.2721 - val_acc: 0.0339
Epoch 3/100
526/526 [==============================] - 0s - loss: 10.6419 - acc: 0.0152 - val_loss: 12.2676 - val_acc: 0.0339
Epoch 4/100
526/526 [==============================] - 0s - loss: 10.8063 - acc: 0.0152 - val_loss: 12.5090 - val_acc: 0.0339
Epoch 5/100
526/526 [==============================] - 0s - loss: 10.7496 - acc: 0.0152 - val_loss: 12.2720 - val_acc: 0.0339
Epoch 6/100
526/526 [==============================] - 0s - loss: 10.6425 - acc: 0.0152 - val_loss: 12.2726 - val_acc: 0.0339
Epoch 7/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2729 - val_acc: 0.0339
Epoch 8/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 9/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0152 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 10/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 11/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 12/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 13/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 14/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 15/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 16/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 17/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 18/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 19/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 20/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 21/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 22/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 23/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 24/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 25/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 26/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 27/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 28/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 29/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 30/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 31/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 32/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 33/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 34/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 35/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 36/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 37/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 38/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 39/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 40/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 41/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 42/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 43/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 44/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 45/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 46/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 47/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 48/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 49/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 50/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 51/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 52/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 53/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 54/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 55/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 56/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 57/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 58/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 59/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 60/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 61/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 62/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 63/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 64/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 65/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 66/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 67/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 68/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 69/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 70/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 71/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 72/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 73/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 74/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 75/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 76/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 77/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 78/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 79/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 80/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 81/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 82/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 83/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 84/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 85/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 86/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 87/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 88/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 89/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 90/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 91/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 92/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 93/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 94/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 95/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 96/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 97/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 98/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 99/100
526/526 [==============================] - 0s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
Epoch 100/100
526/526 [==============================] - 1s - loss: 10.6429 - acc: 0.0133 - val_loss: 12.2733 - val_acc: 0.0339
32/59 [===============>..............] - ETA: 0s
Accuracy : 0.0338983053373



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Dataset : Yale Database 
-------------------------------------------------------------------------


Images length : 153
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 512)               11520512  
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 14)                7182      
_________________________________________________________________
activation_3 (Activation)    (None, 14)                0         
=================================================================
Total params: 11,790,350
Trainable params: 11,790,350
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\MONIK\Anaconda3\lib\site-packages\keras\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  warnings.warn('The `nb_epoch` argument in `fit` '
Train on 137 samples, validate on 16 samples
Epoch 1/50
137/137 [==============================] - 4s - loss: 7.7289 - acc: 0.0657 - val_loss: 13.9477 - val_acc: 0.1250
Epoch 2/50
137/137 [==============================] - 1s - loss: 13.7305 - acc: 0.1095 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 3/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 4/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 5/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 6/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 7/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 8/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 9/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 10/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 11/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 12/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 13/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 14/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 15/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 16/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 17/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 18/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 19/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 20/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 21/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 22/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 23/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 24/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 25/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 26/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 27/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 28/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 29/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 30/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 31/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 32/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 33/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 34/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 35/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 36/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 37/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 38/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 39/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 40/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 41/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 42/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 43/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 44/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 45/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 46/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 47/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 48/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 49/50
137/137 [==============================] - 1s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Epoch 50/50
137/137 [==============================] - 2s - loss: 14.9416 - acc: 0.0730 - val_loss: 15.1107 - val_acc: 0.0625
Loss : 15.1107158661
Accuracy :0.0625